<div class="container">

<table style="width: 100%;"><tr>
<td>gbm.more</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generalized Boosted Regression Modeling (GBM)</h2>

<h3>Description</h3>

<p>Adds additional trees to a <code>gbm.object</code> object.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gbm.more(
  object,
  n.new.trees = 100,
  data = NULL,
  weights = NULL,
  offset = NULL,
  verbose = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>A <code>gbm.object</code> object created from an initial call 
to <code>gbm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.new.trees</code></td>
<td>
<p>Integer specifying the number of additional trees to add 
to <code>object</code>. Default is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>An optional data frame containing the variables in the model. By
default the variables are taken from <code>environment(formula)</code>, typically
the environment from which <code>gbm</code> is called. If <code>keep.data=TRUE</code> in
the initial call to <code>gbm</code> then <code>gbm</code> stores a copy with the
object. If <code>keep.data=FALSE</code> then subsequent calls to
<code>gbm.more</code> must resupply the same dataset. It becomes the user's
responsibility to resupply the same data at this point.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>An optional vector of weights to be used in the fitting
process. Must be positive but do not need to be normalized. If
<code>keep.data=FALSE</code> in the initial call to <code>gbm</code> then it is the
user's responsibility to resupply the weights to <code>gbm.more</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>
<p>A vector of offset values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Logical indicating whether or not to print out progress and 
performance indicators (<code>TRUE</code>). If this option is left unspecified for 
<code>gbm.more</code>, then it uses <code>verbose</code> from <code>object</code>. Default is
<code>FALSE</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A <code>gbm.object</code> object.
</p>


<h3>Examples</h3>

<pre><code class="language-R">#
# A least squares regression example 
#

# Simulate data
set.seed(101)  # for reproducibility
N &lt;- 1000
X1 &lt;- runif(N)
X2 &lt;- 2 * runif(N)
X3 &lt;- ordered(sample(letters[1:4], N, replace = TRUE), levels = letters[4:1])
X4 &lt;- factor(sample(letters[1:6], N, replace = TRUE))
X5 &lt;- factor(sample(letters[1:3], N, replace = TRUE))
X6 &lt;- 3 * runif(N) 
mu &lt;- c(-1, 0, 1, 2)[as.numeric(X3)]
SNR &lt;- 10  # signal-to-noise ratio
Y &lt;- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu
sigma &lt;- sqrt(var(Y) / SNR)
Y &lt;- Y + rnorm(N, 0, sigma)
X1[sample(1:N,size=500)] &lt;- NA  # introduce some missing values
X4[sample(1:N,size=300)] &lt;- NA  # introduce some missing values
data &lt;- data.frame(Y, X1, X2, X3, X4, X5, X6)

# Fit a GBM
set.seed(102)  # for reproducibility
gbm1 &lt;- gbm(Y ~ ., data = data, var.monotone = c(0, 0, 0, 0, 0, 0),
            distribution = "gaussian", n.trees = 100, shrinkage = 0.1,             
            interaction.depth = 3, bag.fraction = 0.5, train.fraction = 0.5,  
            n.minobsinnode = 10, cv.folds = 5, keep.data = TRUE, 
            verbose = FALSE, n.cores = 1)  

# Check performance using the out-of-bag (OOB) error; the OOB error typically
# underestimates the optimal number of iterations
best.iter &lt;- gbm.perf(gbm1, method = "OOB")
print(best.iter)

# Check performance using the 50% heldout test set
best.iter &lt;- gbm.perf(gbm1, method = "test")
print(best.iter)

# Check performance using 5-fold cross-validation
best.iter &lt;- gbm.perf(gbm1, method = "cv")
print(best.iter)

# Plot relative influence of each variable
par(mfrow = c(1, 2))
summary(gbm1, n.trees = 1)          # using first tree
summary(gbm1, n.trees = best.iter)  # using estimated best number of trees

# Compactly print the first and last trees for curiosity
print(pretty.gbm.tree(gbm1, i.tree = 1))
print(pretty.gbm.tree(gbm1, i.tree = gbm1$n.trees))

# Simulate new data
set.seed(103)  # for reproducibility
N &lt;- 1000
X1 &lt;- runif(N)
X2 &lt;- 2 * runif(N)
X3 &lt;- ordered(sample(letters[1:4], N, replace = TRUE))
X4 &lt;- factor(sample(letters[1:6], N, replace = TRUE))
X5 &lt;- factor(sample(letters[1:3], N, replace = TRUE))
X6 &lt;- 3 * runif(N) 
mu &lt;- c(-1, 0, 1, 2)[as.numeric(X3)]
Y &lt;- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu + rnorm(N, 0, sigma)
data2 &lt;- data.frame(Y, X1, X2, X3, X4, X5, X6)

# Predict on the new data using the "best" number of trees; by default,
# predictions will be on the link scale
Yhat &lt;- predict(gbm1, newdata = data2, n.trees = best.iter, type = "link")

# least squares error
print(sum((data2$Y - Yhat)^2))

# Construct univariate partial dependence plots
plot(gbm1, i.var = 1, n.trees = best.iter)
plot(gbm1, i.var = 2, n.trees = best.iter)
plot(gbm1, i.var = "X3", n.trees = best.iter)  # can use index or name

# Construct bivariate partial dependence plots
plot(gbm1, i.var = 1:2, n.trees = best.iter)
plot(gbm1, i.var = c("X2", "X3"), n.trees = best.iter)
plot(gbm1, i.var = 3:4, n.trees = best.iter)

# Construct trivariate partial dependence plots
plot(gbm1, i.var = c(1, 2, 6), n.trees = best.iter, 
     continuous.resolution = 20)
plot(gbm1, i.var = 1:3, n.trees = best.iter)
plot(gbm1, i.var = 2:4, n.trees = best.iter)
plot(gbm1, i.var = 3:5, n.trees = best.iter)

# Add more (i.e., 100) boosting iterations to the ensemble
gbm2 &lt;- gbm.more(gbm1, n.new.trees = 100, verbose = FALSE)
</code></pre>


</div>