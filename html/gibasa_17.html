<div class="container">

<table style="width: 100%;"><tr>
<td>lex_density</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate lexical density</h2>

<h3>Description</h3>

<p>The lexical density is the proportion of content words (lexical items)
in documents. This function is a simple helper for calculating
the lexical density of given datasets.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lex_density(vec, contents_words, targets = NULL, negate = c(FALSE, FALSE))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>vec</code></td>
<td>
<p>A character vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>contents_words</code></td>
<td>
<p>A character vector containing values
to be counted as contents words.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>targets</code></td>
<td>
<p>A character vector with which
the denominator of lexical density is filtered before computing values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negate</code></td>
<td>
<p>A logical vector of which length is 2.
If passed as <code>TRUE</code>, then respectively negates the predicate functions
for counting contents words or targets.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A numeric vector.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
df &lt;- tokenize(
  data.frame(
    doc_id = seq_along(ginga[5:8]),
    text = ginga[5:8]
  )
)
df |&gt;
  prettify(col_select = "POS1") |&gt;
  dplyr::group_by(doc_id) |&gt;
  dplyr::summarise(
    noun_ratio = lex_density(POS1,
      "\u540d\u8a5e",
      c("\u52a9\u8a5e", "\u52a9\u52d5\u8a5e"),
      negate = c(FALSE, TRUE)
    ),
    mvr = lex_density(
      POS1,
      c("\u5f62\u5bb9\u8a5e", "\u526f\u8a5e", "\u9023\u4f53\u8a5e"),
      "\u52d5\u8a5e"
    ),
    vnr = lex_density(POS1, "\u52d5\u8a5e", "\u540d\u8a5e")
  )

## End(Not run)
</code></pre>


</div>