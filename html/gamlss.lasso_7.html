<div class="container">

<table style="width: 100%;"><tr>
<td>lrs</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Least angle regression and lasso in GAMLSS
</h2>

<h3>Description</h3>

<p>This function allows estimating the different components of a GAMLSS model (mean, sd. dev., skewness and kurtosis) using the elastic net (with lasso as default special case) estimation method via glmnet.  This method is appropriate for models with many variables.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lrs(X = NULL, x.vars = NULL, lambda = NULL,	method = c("IC","CV"), 
  type = c("agg","sel"), ICpen = c("BIC", "HQC", "AIC"), CVp = 2, k.se = 0, 
  subsets = NULL, lars.type= "lasso", use.gram = TRUE, 
  eps = .Machine$double.eps, max.steps = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p> The data frame containing the explanatory variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.vars</code></td>
<td>
<p> Indicates the name of the variables that must be included as explanatory variables from data the data object of GAMLSS. The explanatory variables must be included by <code>X</code> or by <code>x.vars</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p> The provided lambda grid. By default <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p> The method used to calculate the optimal lambda. If <code>method="IC"</code> information criteria are used, the penalization for the information criterion is selected in <code>ICpen</code>.If <code>method="CV"</code> cross validation resp. sampling is used, the penalization for the cross-validation is selected in <code>CVp</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p> The way to select the optimal lambda across the subsample fits. If <code>type="sel"</code> the optimal lambda is computed by selection. If <code>method="agg"</code> the optimal lambda is computed by aggregation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ICpen</code></td>
<td>
<p> The penalization for the information criteria. If <code>ICpen="AIC"</code> or  <code>ICpen=2</code> the optimal lambda is computed by Akaike Information Criterion. If <code>ICpen="BIC"</code> the optimal lambda is computed by Bayesian Information Criterion.If <code>ICpen="HQC"</code> the optimal lambda is computed by Hannan-Quinn Information Criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CVp</code></td>
<td>
<p> The penalization for the cross-validation, establishes the power of the error term. By default is equal to 2, i.e. squared error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k.se</code></td>
<td>
<p> This parameter establishes how many times the standard deviation is summed to the mean to select the optimal lambda. By default is equal to 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subsets</code></td>
<td>
<p> The subsets for cross-validation, information criteria or bootstraping, by default 5 random fold are selected.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lars.type</code></td>
<td>
<p>As in <code>lars</code>, lars type, e.g. "lasso", "lar" (least angle regression), "forward.stagewise" or "stepwise".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use.gram</code></td>
<td>
<p>States if Gramian should be precomputed, default TRUE - recommended as gamlss will call lars often during the estimation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>As in <code>lars</code>, a small constant.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.steps</code></td>
<td>
<p>As in <code>lars</code>, number of updating steps (for "lars" method equal to number of variables, for "lasso" it can be smaller), default NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>for extra arguments</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The estimation of the lambda is carried out by BIC by default.
If the objective is to predict the model must be defined by <code>x.vars</code>. 
Different types of subsets must be constructed if bootstrapping and aggregation are applied, as in this case observations might be repeated. 

</p>


<h3>Value</h3>

<p>This function returns a smooth object of the GAMLSS model. It contains the estimated parameters and related characteristics for the <code>lars</code> component in the GAMLSS model we are estimating.
</p>


<h3>Author(s)</h3>

<p>Florian Ziel, Peru Muniain and Mikis Stasinopoulos
</p>


<h3>References</h3>

<p>Rigby, R. A. and  Stasinopoulos D. M. (2005). Generalized additive models for location, scale, and shape,(with discussion), <em>Appl. Statist.</em>, <b>54</b>, part 3, pp 507-554.
</p>
<p>Rigby, R. A., Stasinopoulos, D. M., Heller, G. Z., and De Bastiani, F. (2019) Distributions for modeling location, scale, and shape: Using GAMLSS in R, Chapman and Hall/CRC. An older version can be found in https://www.gamlss.com/. 
</p>
<p>Stasinopoulos D. M. Rigby R.A. (2007) Generalized additive models for location scale and shape (GAMLSS) in R. <em>Journal of Statistical Software</em>, <b>Vol. 23</b>, Issue 7, Dec 2007, https://www.jstatsoft.org/v23/i07/. 
</p>
<p>Stasinopoulos D. M., Rigby R.A., Heller G., Voudouris V., and De Bastiani F., (2017) Flexible Regression and Smoothing: Using GAMLSS in R, Chapman and Hall/CRC. 
</p>
<p>Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011) Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent, <em>Journal of Statistical Software</em>, <b>Vol. 39(5)</b>, 1-13, https://www.jstatsoft.org/v39/i05/.
</p>
<p>Tibshirani, Robert, Bien, J., Friedman, J., Hastie, T.,Simon, N.,Taylor, J. and Tibshirani, Ryan. (2012) Strong Rules for Discarding Predictors in Lasso-type Problems, <em>JRSSB</em>, <b>Vol. 74(2)</b>, 245-266, https://statweb.stanford.edu/~tibs/ftp/strong.pdf.
</p>
<p>Hastie, T., Tibshirani, Robert and Tibshirani, Ryan. Extended Comparisons of Best Subset Selection, Forward Stepwise Selection, and the Lasso (2017), <em>Stanford Statistics Technical Report</em>, https://arxiv.org/abs/1707.08692.
</p>
<p>Efron, Hastie, Johnstone and Tibshirani (2003) "Least Angle Regression"
(with discussion) <em>Annals of Statistics</em>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Contructing the data
library(gamlss.lasso)
set.seed(123)
n&lt;- 500
d&lt;- 50
X&lt;- matrix(rnorm(n*d), n,d)
BETA&lt;- cbind( "mu"=rbinom(d,1,.1), "sigma"= rbinom(d,1,.1)*.3)
ysd&lt;- exp(1 + tcrossprod( BETA[,2],X))
data&lt;- cbind(y=as.numeric(rnorm(n,sd=ysd)) + t(tcrossprod( BETA[,1],X)),as.data.frame(X))

# Estimating the model with lrs default setting
mod &lt;- gamlss(y~lrs(x.vars=names(data)[-1] ),
              sigma.fo=~lrs(x.vars=names(data)[-1]), data=data, family=NO,
              i.control = glim.control(cyc=1, bf.cyc=1))

# Estimated paramters are available at
rbind(true=BETA[,1],estimate=tail(getSmo(mod, "mu") ,1)[[1]]$beta )## beta for mu
rbind(true=BETA[,2],estimate=tail(getSmo(mod, "sigma") ,1)[[1]]$beta )## beta for sigma

</code></pre>


</div>