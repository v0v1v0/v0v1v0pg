<div class="container">

<table style="width: 100%;"><tr>
<td>gglasso</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fits the regularization paths for group-lasso penalized learning problems</h2>

<h3>Description</h3>

<p>Fits regularization paths for group-lasso penalized learning problems at a
sequence of regularization parameters lambda.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gglasso(
  x,
  y,
  group = NULL,
  loss = c("ls", "logit", "sqsvm", "hsvm", "wls"),
  nlambda = 100,
  lambda.factor = ifelse(nobs &lt; nvars, 0.05, 0.001),
  lambda = NULL,
  pf = sqrt(bs),
  weight = NULL,
  dfmax = as.integer(max(group)) + 1,
  pmax = min(dfmax * 1.2, as.integer(max(group))),
  eps = 1e-08,
  maxit = 3e+08,
  delta,
  intercept = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>matrix of predictors, of dimension <code class="reqn">n \times p</code>; each row
is an observation vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>response variable. This argument should be quantitative for
regression (least squares), and a two-level factor for classification
(logistic model, huberized SVM, squared SVM).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>group</code></td>
<td>
<p>a vector of consecutive integers describing the grouping of the
coefficients (see example below).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>a character string specifying the loss function to use, valid
options are: </p>
 <ul>
<li> <p><code>"ls"</code> least squares loss (regression),
</p>
</li>
<li> <p><code>"logit"</code> logistic loss (classification).  </p>
</li>
<li> <p><code>"hsvm"</code>
Huberized squared hinge loss (classification), </p>
</li>
<li> <p><code>"sqsvm"</code> Squared
hinge loss (classification), </p>
</li>
</ul>
<p>Default is <code>"ls"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>the number of <code>lambda</code> values - default is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.factor</code></td>
<td>
<p>the factor for getting the minimal lambda in
<code>lambda</code> sequence, where <code>min(lambda)</code> = <code>lambda.factor</code> *
<code>max(lambda)</code>.  <code>max(lambda)</code> is the smallest value of
<code>lambda</code> for which all coefficients are zero. The default depends on
the relationship between <code class="reqn">n</code> (the number of rows in the matrix of
predictors) and <code class="reqn">p</code> (the number of predictors). If <code class="reqn">n &gt;= p</code>, the
default is <code>0.001</code>, close to zero.  If <code class="reqn">n&lt;p</code>, the default is
<code>0.05</code>.  A very small value of <code>lambda.factor</code> will lead to a
saturated fit. It takes no effect if there is user-defined <code>lambda</code>
sequence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>a user supplied <code>lambda</code> sequence. Typically, by leaving
this option unspecified users can have the program compute its own
<code>lambda</code> sequence based on <code>nlambda</code> and <code>lambda.factor</code>.
Supplying a value of <code>lambda</code> overrides this. It is better to supply a
decreasing sequence of <code>lambda</code> values than a single (small) value, if
not, the program will sort user-defined <code>lambda</code> sequence in
decreasing order automatically.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pf</code></td>
<td>
<p>penalty factor, a vector in length of bn (bn is the total number of
groups). Separate penalty weights can be applied to each group of
<code class="reqn">\beta</code>s to allow differential shrinkage. Can be 0 for some
groups, which implies no shrinkage, and results in that group always being
included in the model. Default value for each entry is the square-root of
the corresponding size of each group.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight</code></td>
<td>
<p>a <code class="reqn">nxn</code> observation weight matrix in the where <code class="reqn">n</code> is
the number of observations. Only used if <code>loss='wls'</code> is specified.
Note that cross-validation is NOT IMPLEMENTED for <code>loss='wls'</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfmax</code></td>
<td>
<p>limit the maximum number of groups in the model. Useful for very
large <code>bs</code> (group size), if a partial path is desired. Default is
<code>bs+1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pmax</code></td>
<td>
<p>limit the maximum number of groups ever to be nonzero. For
example once a group enters the model, no matter how many times it exits or
re-enters model through the path, it will be counted only once. Default is
<code>min(dfmax*1.2,bs)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>convergence termination tolerance. Defaults value is <code>1e-8</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>maximum number of outer-loop iterations allowed at fixed lambda
value. Default is 3e8. If models do not converge, consider increasing
<code>maxit</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>the parameter <code class="reqn">\delta</code> in <code>"hsvm"</code> (Huberized
squared hinge loss). Default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>Whether to include intercept in the model. Default is TRUE.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Note that the objective function for <code>"ls"</code> least squares is
</p>
<p style="text-align: center;"><code class="reqn">RSS/(2*n) + lambda * penalty;</code>
</p>
<p> for <code>"hsvm"</code> Huberized squared
hinge loss, <code>"sqsvm"</code> Squared hinge loss and <code>"logit"</code> logistic
regression, the objective function is </p>
<p style="text-align: center;"><code class="reqn">-loglik/n + lambda * penalty.</code>
</p>

<p>Users can also tweak the penalty by choosing different penalty factor.
</p>
<p>For computing speed reason, if models are not converging or running slow,
consider increasing <code>eps</code>, decreasing <code>nlambda</code>, or increasing
<code>lambda.factor</code> before increasing <code>maxit</code>.
</p>


<h3>Value</h3>

<p>An object with S3 class <code>gglasso</code>.  </p>
<table>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the call
that produced this object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b0</code></td>
<td>
<p>intercept sequence of length
<code>length(lambda)</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>a <code>p*length(lambda)</code> matrix of
coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>the number of nonzero groups for each value of
<code>lambda</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>dimension of coefficient matrix (ices)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>the actual sequence of <code>lambda</code> values used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>npasses</code></td>
<td>
<p>total number of iterations (the most inner loop) summed over
all lambda values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jerr</code></td>
<td>
<p>error flag, for warnings and errors, 0 if no
error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>group</code></td>
<td>
<p>a vector of consecutive integers describing the
grouping of the coefficients.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Yi Yang and Hui Zou<br> Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2015), “A Fast Unified Algorithm for
Computing Group-Lasso Penalized Learning Problems,” <em>Statistics and
Computing</em>. 25(6), 1129-1141.<br> BugReport:
<a href="https://github.com/emeryyi/gglasso">https://github.com/emeryyi/gglasso</a><br></p>


<h3>See Also</h3>

<p><code>plot.gglasso</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# load gglasso library
library(gglasso)

# load bardet data set
data(bardet)

# define group index
group1 &lt;- rep(1:20,each=5)

# fit group lasso penalized least squares
m1 &lt;- gglasso(x=bardet$x,y=bardet$y,group=group1,loss="ls")

# load colon data set
data(colon)

# define group index
group2 &lt;- rep(1:20,each=5)

# fit group lasso penalized logistic regression
m2 &lt;- gglasso(x=colon$x,y=colon$y,group=group2,loss="logit")

</code></pre>


</div>