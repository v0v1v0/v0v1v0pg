<div class="container">

<table style="width: 100%;"><tr>
<td>RED</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Redundancy Rate
</h2>

<h3>Description</h3>

<p>Calculate the redundancy rate of the selected features(markers). Value will be high if many redundant features are selected.
</p>


<h3>Usage</h3>

<pre><code class="language-R">RED(x,spam_selected_feature_index,hsic_selected_feature_index,
integrated_selected_feature_index)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a matrix of markers or explanatory variables, each column contains one marker and each row represents an individual.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>spam_selected_feature_index</code></td>
<td>
<p>index of selected markers from x using Sparse Additive Model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hsic_selected_feature_index</code></td>
<td>
<p>index of selected markers from x using HSIC LASSO.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>integrated_selected_feature_index</code></td>
<td>
<p>index of selected markers from x using integrated model framework</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The RED score (Zhao et al., 2010) is determined by average of the correlation between each pair of selected markers. A large RED score signifies that selected features are more strongly correlated to each other which means many redundant features are selected. Thus, a small redundancy rate is preferable for feature selection.
</p>


<h3>Value</h3>

<p>Returns a LIST containing
</p>
<table>
<tr style="vertical-align: top;">
<td><code>RED_spam</code></td>
<td>
<p>returns redundancy rate of features selected by using Sparse Additive Model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>RED_hsic</code></td>
<td>
<p>returns redundancy rate of features selected by using HSIC LASSO.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>RED_I</code></td>
<td>
<p>returns redundancy rate of features selected by using integrated model framework.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Sayanti Guha Majumdar &lt;<a href="mailto:sayanti23gm@gmail.com">sayanti23gm@gmail.com</a>&gt;, Anil Rai, Dwijesh Chandra Mishra
</p>


<h3>References</h3>

<p>Guha Majumdar, S., Rai, A. and Mishra, D. C. (2019). Integrated framework for selection of additive and non-additive genetic markers for genomic selection. <em>Journal of Computational Biology</em>. doi:10.1089/cmb.2019.0223
<br> Ravikumar, P., Lafferty, J., Liu, H. and Wasserman, L. (2009). Sparse additive models. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 71(5), 1009-1030. doi:10.1111/j.1467-9868.2009.00718.x
<br> Yamada, M., Jitkrittum, W., Sigal, L., Xing, E. P. and Sugiyama, M. (2014). High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso. <em>Neural Computation</em>, 26(1):185-207. doi:10.1162/NECO_a_00537
<br> Zhao, Z., Wang, L. and Li, H. (2010). Efficient spectral feature selection with minimum redundancy. <em>In AAAI Conference on Artificial Intelligence (AAAI)</em>, pp 673-678.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(GSelection)
data(GS)
x_trn &lt;- GS[1:40,1:110]
y_trn &lt;- GS[1:40,111]
x_tst &lt;- GS[41:60,1:110]
y_tst &lt;- GS[41:60,111]
fit &lt;- feature.selection(x_trn,y_trn,d=10)
red &lt;- RED(x_trn,fit$spam_selected_feature_index,fit$hsic_selected_feature_index,
fit$integrated_selected_feature_index)
</code></pre>


</div>