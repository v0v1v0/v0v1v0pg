<div class="container">

<table style="width: 100%;"><tr>
<td>gKRLS</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generalized Kernel Regularized Least Squares</h2>

<h3>Description</h3>

<p>This page documents how to use <code>gKRLS</code> as part of a model estimated with
<code>mgcv</code>. Post-estimation functions to calculate marginal effects are
documented elsewhere, e.g. calculate_effects.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gKRLS(
  sketch_method = "subsampling",
  standardize = "Mahalanobis",
  bandwidth = NULL,
  sketch_multiplier = 5,
  sketch_size_raw = NULL,
  sketch_prob = NULL,
  rescale_penalty = TRUE,
  truncate.eigen.tol = sqrt(.Machine$double.eps),
  demean_kernel = FALSE,
  remove_instability = TRUE
)

get_calibration_information(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>sketch_method</code></td>
<td>
<p>A string that specifies which kernel sketching method
should be used (default of <code>"subsampling"</code>). Options include
<code>"subsampling"</code>, <code>"gaussian"</code>, <code>"bernoulli"</code>, or
<code>"none"</code> (no sketching). Drineas et al. (2005) and Yang et al. (2017)
provide more details on these options.
</p>
<p>To force <code>"subsampling"</code> to select a specific set of observations, you
can provide a vector of row positions to <code>sketch_method</code>. This
manually sets the size of the sketching multiplier, implicitly overriding
other options in <code>gKRLS</code>. The examples provide an illustration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>A string that specifies how the data is standardized
before calculating the distance between observations. The default is
<code>"Mahalanobis"</code> (i.e., demeaned and transformed to have an identity
covariance matrix). Other options are <code>"scaled"</code> (all columns are
scaled to have mean zero and variance of one) or <code>"none"</code> (no
standardization).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bandwidth</code></td>
<td>
<p>A bandwidth <code class="reqn">P</code> for the kernel where each element of the
kernel <code class="reqn">(i,j)</code> is defined by <code class="reqn">\exp(-||x_i - x_j||^2_2/P)</code>. The
default (<code>NULL</code>) uses the number of covariates in the kernel or the
rank of the corresponding design matrix. An additional option
(<code>"calibrate"</code>) choses <code class="reqn">P</code> to maximize the variance of the kernel,
e.g., <code class="reqn">var(vec(K))</code> for the unsketched case. This follows Hartman et
al. (2024) with modifications when the kernel is sketched. Please see
<a href="https://github.com/mgoplerud/gKRLS/blob/master/.github/gKRLS_addendum.pdf">gKRLS_addendum.pdf</a>
for a formal exposition.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sketch_multiplier</code></td>
<td>
<p>A number that sets the size of the sketching
dimension: <code>sketch_multiplier * ceiling(N^(1/3))</code> where <code>N</code> is
the number of observations. The default is 5; Chang and Goplerud (2024)
find that increasing this to 15 may improve stability for certain complex
kernels. <code>sketch_size_raw</code> can directly set the size of the sketching
dimension.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sketch_size_raw</code></td>
<td>
<p>A number to set the exact size of the sketching
dimension. The default, <code>NULL</code>, means that this argument is not used
and the size depends on the number of observations; see
<code>sketch_multiplier</code>. Exactly one of <code>sketch_size_raw</code> or
<code>sketch_multiplier</code> must be <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sketch_prob</code></td>
<td>
<p>A probability for an element of the sketching matrix to
equal <code>1</code> when using Bernoulli sketching. Yang et al. (2017) provide
more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rescale_penalty</code></td>
<td>
<p>A logical value for whether the penalty should be
rescaled for numerical stability. See documentation for
<code>mgcv::smooth.spec</code> on the meaning of this term. The default is
<code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>truncate.eigen.tol</code></td>
<td>
<p>A threshold to remove columns of the penalty
<code class="reqn">S K S^T</code> whose eigenvalues are small (below
<code>truncate.eigen.tol</code>). These columns are removed from the sketched
kernel and avoids instability due to numerically very small eigenvalues. The
default is <code>sqrt(.Machine$double.eps)</code>. This adjustment can be
disabled by setting <code>remove_instability = FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>demean_kernel</code></td>
<td>
<p>A logical value that indicates whether columns of the
(sketched) kernel should be demeaned before estimation. The default is
<code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>remove_instability</code></td>
<td>
<p>A logical value that indicates whether numerical
zeros (set via <code>truncate.eigen.tol</code>) should be removed when building
the penalty matrix. The default is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Model estimated using <code>mgcv::gam</code> or <code>mgcv::bam</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><b>Overview:</b> The <code>gKRLS</code> function should not be called directly. Its
options, described above, control how <code>gKRLS</code> is estimated. It should be
passed to <code>mgcv</code> as follows: <code>s(x1, x2, x3, bs = "gKRLS", xt =
gKRLS(...))</code>. Multiple kernels can be specified and have different
<code>gKRLS</code> arguments. It can also be used alongside the existing options
for <code>s()</code> in <code>mgcv</code>.
</p>
<p>If <code>bandwidth="calibrate"</code>, the function
<code>get_calibration_information</code> reports the estimated bandwidth and time
(in minutes) needed to do so.
</p>
<p><b>Default Settings:</b> By default, <code>bs = "gKRLS"</code> uses Mahalanobis
distance between the observations, random sketching using subsampling
sketching (i.e., where the kernel is constructed using a random sample of the
observations; Yang et al. 2017) and a sketching dimension of <code>5 *
ceiling(N^(1/3))</code> where <code>N</code> is the number of observations. Chang and
Goplerud (2024) provide an exploration of alternative options.
</p>
<p><b>Notes:</b> Please note that variables must be separated with commas inside
of <code>s(...)</code> and that character variables should usually be passed as
factors to work smoothly with <code>mgcv</code>. When using this function with
<code>bam</code>, the sketching dimension uses <code>chunk.size</code> in place of
<code>N</code> and thus either <code>chunk.size</code> or <code>sketch_size_raw</code> must be used to cause
the sketching dimension to increase with <code>N</code>.
</p>


<h3>Value</h3>

<p><code>gKRLS</code> returns a named list with the elements in "Arguments".
</p>


<h3>References</h3>

<p>Chang, Qing, and Max Goplerud. 2024. "Generalized Kernel Regularized Least
Squares." <em>Political Analysis</em> 32(2):157-171.
</p>
<p>Hartman, Erin, Chad Hazlett, and Ciara Sterbenz. 2024. "kpop: A Kernel
Balancing Approach for Reducing Specification Assumptions in Survey
Weighting." <em>Journal of the Royal Statistical Society Series A:
Statistics in Society</em> <a href="https://doi.org/10.1093/jrsssa/qnae082">doi:10.1093/jrsssa/qnae082</a>.
</p>
<p>Drineas, Petros, Michael W. Mahoney, and Nello Cristianini. 2005. "On the
Nystr√∂m Method for Approximating a Gram Matrix For Improved Kernel-Based
Learning." <em>Journal of Machine Learning Research</em> 6(12):2153-2175.
</p>
<p>Yang, Yun, Mert Pilanci, and Martin J. Wainwright. 2017. "Randomized
Sketches for Kernels: Fast and Optimal Nonparametric Regression."
<em>Annals of Statistics</em> 45(3):991-1023.
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(123)
n &lt;- 100
x1 &lt;- rnorm(n)
x2 &lt;- rnorm(n)
x3 &lt;- rnorm(n)
state &lt;- sample(letters[1:5], n, replace = TRUE)
y &lt;- 0.3 * x1 + 0.4 * x2 + 0.5 * x3 + rnorm(n)
data &lt;- data.frame(y, x1, x2, x3, state)
data$state &lt;- factor(data$state)
# A gKRLS model without fixed effects
fit_gKRLS &lt;- mgcv::gam(y ~ s(x1, x2, x3, bs = "gKRLS"), data = data)
summary(fit_gKRLS)
# A gKRLS model with fixed effects outside of the kernel
fit_gKRLS_FE &lt;- mgcv::gam(y ~ state + s(x1, x2, x3, bs = "gKRLS"), data = data)

# HC3 is not available for mgcv; this uses the effective degrees of freedom
# instead of the number of columns; see ?estfun.gam for details
robust &lt;- sandwich::vcovHC(fit_gKRLS, type = 'HC1')
cluster &lt;- sandwich::vcovCL(fit_gKRLS, cluster = data$state)

# Change default standardization to "scaled", sketch method to Gaussian,
# and alter sketching multiplier
fit_gKRLS_alt &lt;- mgcv::gam(y ~ s(x1, x2, x3,
  bs = "gKRLS",
  xt = gKRLS(
    standardize = "scaled",
    sketch_method = "gaussian",
    sketch_multiplier = 2
  )
),
data = data
)
# A model with multiple kernels
fit_gKRLS_2 &lt;- mgcv::gam(y ~ s(x1, x2, bs = 'gKRLS') + s(x1, x3, bs = 'gKRLS'), data = data)
# A model with a custom set of ids for sketching
id &lt;- sample(1:n, 5)
fit_gKRLS_custom &lt;- mgcv::gam(y ~ s(x1, bs = 'gKRLS', xt = gKRLS(sketch_method = id)), data = data)
# Note that the ids of the sampled observations can be extracted 
# from the fitted mgcv object
stopifnot(identical(id, fit_gKRLS_custom$smooth[[1]]$subsampling_id))
# calculate marginal effect (see ?calculate_effects for more examples)
calculate_effects(fit_gKRLS, variables = "x1")
</code></pre>


</div>