<div class="container">

<table style="width: 100%;"><tr>
<td>cv.glmertree</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cross Validation of (Generalized) Linear Mixed Model Trees</h2>

<h3>Description</h3>

<p>Performs cross-validation of a model-based recursive partition based on (generalized)
linear mixed models. Using the tree or subgroup structure estimated from a training dataset,
the full mixed-effects model parameters are re-estimated using a new set of test observations,
providing valid computation of standard errors and valid inference. The approach is inspired
by Athey &amp; Imbens (2016), and "enables the construction of valid confidence intervals [...] whereby 
one sample is used to construct the partition and another to estimate [...] effects for each 
subpopulation."
</p>


<h3>Usage</h3>

<pre><code class="language-R">cv.lmertree(tree, newdata, reference = NULL, omit.intercept = FALSE, ...)

cv.glmertree(tree, newdata, reference = NULL, omit.intercept = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>tree</code></td>
<td>
<p>An object of class <code>lmertree</code> or <code>glmertree</code> that was fitted on 
a set of training data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p>A <code>data.frame</code> containing a new set of observations on the same
variables that were used to fit <code>tree</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reference</code></td>
<td>
<p>Numeric or character scalar, indicating the number of the terminal node of 
which the intercept should be taken as a reference for intercepts in all other nodes. 
If <code>NULL</code>, the default of taking the first terminal node's intercept as the reference category
will be used. If the interest is in testing significance of differences between the different
nodes intercepts, this can be overruled by specifying the number of the terminal node
that should be used as the reference category.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>omit.intercept</code></td>
<td>
<p>Logical scalar, indicating whether the intercept should be omitted from the model.
The default (<code>FALSE</code>) includes the intercept of the first terminal node as the intercept and allows
for significance testing of the differences between the first and the other terminal node's intercepts.
Specifying <code>TRUE</code> will test the value of each terminal node's intercept against zero.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Not currently used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The approach is inspired
by Athey &amp; Imbens (2016), and "enables the construction of valid confidence intervals [...] whereby 
one sample is used to construct the partition and another to estimate [...] effects for each 
subpopulation."</p>


<h3>Value</h3>

<p>An object of with classes <code>lmertree</code> and <code>cv.lmertree</code>, or <code>glmertree</code> and <code>cv.glmertree</code>. It is the original (g)lmertree specified by argument <code>tree</code>, but the parametric model model estimated based on the data specified by argument <code>newdata</code>. The default S3 methods for classes <code>lmertree</code> and <code>glmertree</code> can be used to inspect the results: <code>plot</code>, <code>predict</code>, <code>coef</code>, <code>fixef</code>, <code>ranef</code> and <code>VarCorr</code>. In addition, there is a dedicated <code>summary</code> method for classes <code>cv.lmertree</code> and <code>cv.glmertree</code>, which prints valid parameter estimates and standard errors, resulting from <code>summary.merMod</code>. For objects of clas <code>cv.lmertree</code>, hypothesis tests (i.e., p-values) can be obtained by loading package <code>lmerTest</code> PRIOR to loading package(s) <code>glmertree</code> (and <code>lme4</code>), see examples.
</p>


<h3>References</h3>

<p>Athey S &amp; Imbens G (2016). Recursive partitioning for heterogeneous causal effects. 
“Proceedings of the National Academy of Sciences, 113(27), 7353-7360.”
<a href="https://doi.org/10.1073/pnas.1510489113">doi:10.1073/pnas.1510489113</a>
</p>
<p>Fokkema M, Smits N, Zeileis A, Hothorn T, Kelderman H (2018).
“Detecting Treatment-Subgroup Interactions in Clustered Data
with Generalized Linear Mixed-Effects Model Trees”. Behavior Research 
Methods, 50(5), 2016-2034. <a href="https://doi.org/10.3758/s13428-017-0971-x">doi:10.3758/s13428-017-0971-x</a>
</p>
<p>Fokkema M, Edbrooke-Childs J &amp; Wolpert M (2021). “Generalized linear mixed-model 
(GLMM) trees: A flexible decision-tree method for multilevel and longitudinal data.” 
Psychotherapy Research, 31(3), 329-341. <a href="https://doi.org/10.1080/10503307.2020.1785037">doi:10.1080/10503307.2020.1785037</a>
</p>
<p>Fokkema M &amp; Zeileis A (2024). Subgroup detection in linear growth curve models with 
generalized linear mixed model (GLMM) trees. Behavior Research Methods. 
<a href="https://doi.org/10.3758/s13428-024-02389-1">doi:10.3758/s13428-024-02389-1</a>
</p>


<h3>See Also</h3>

<p><code>lmer</code>, <code>glmer</code>,
<code>lmertree</code>, <code>glmertree</code>, 
<code>summary.merMod</code></p>


<h3>Examples</h3>

<pre><code class="language-R">

require("lmerTest") ## load BEFORE lme4 and glmertree to obtain hypothesis tests / p-values

## Create artificial training and test datasets
set.seed(42)
train &lt;- sample(1:nrow(DepressionDemo), size = 200, replace = TRUE)
test &lt;- sample(1:nrow(DepressionDemo), size = 200, replace = TRUE)

## Fit tree on training data
tree1 &lt;- lmertree(depression ~ treatment | cluster | age + anxiety + duration,
                 data = DepressionDemo[train, ])
                 
## Obtain honest estimates of parameters and standard errors using test data
tree2 &lt;- cv.lmertree(tree1, newdata = DepressionDemo[test, ])
tree3 &lt;- cv.lmertree(tree1, newdata = DepressionDemo[test, ], 
                     reference = 7, omit.intercept = TRUE)

summary(tree2)
summary(tree3)

coef(tree1)
coef(tree2)
coef(tree3)

plot(tree1, which = "tree")
plot(tree2, which = "tree")
plot(tree3, which = "tree")

predict(tree1, newdata = DepressionDemo[1:5, ])
predict(tree2, newdata = DepressionDemo[1:5, ])
</code></pre>


</div>