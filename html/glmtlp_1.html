<div class="container">

<table style="width: 100%;"><tr>
<td>glmtlp</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization.</h2>

<h3>Description</h3>

<p>The package provides 3 penalties: l0, l1, and tlp and 3 distribution families: 
gaussian, binomial, and poisson.
</p>
<p>Fit generalized linear models via penalized maximum likelihood. The
regularization path is computed for the l0, lasso, or truncated lasso
penalty at a grid of values for the regularization parameter <code>lambda</code>
or <code>kappa</code>. Fits linear and logistic regression models.
</p>


<h3>Usage</h3>

<pre><code class="language-R">glmtlp(
  X,
  y,
  family = c("gaussian", "binomial"),
  penalty = c("l0", "l1", "tlp"),
  nlambda = ifelse(penalty == "l0", 50, 100),
  lambda.min.ratio = ifelse(nobs &lt; nvars, 0.05, 0.001),
  lambda = NULL,
  kappa = NULL,
  tau = 0.3 * sqrt(log(nvars)/nobs),
  delta = 2,
  tol = 1e-04,
  weights = NULL,
  penalty.factor = rep(1, nvars),
  standardize = FALSE,
  dc.maxit = 20,
  cd.maxit = 10000,
  nr.maxit = 20,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Input matrix, of dimension <code>nobs</code> x <code>nvars</code>;
each row is  an observation vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Response variable, of length <code>nobs</code>. For <code>family="gaussian"</code>,
it should be quantitative; for <code>family="binomial"</code>, it should be either
a factor with two levels or a binary vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>A character string representing one of the built-in families.
See Details section below.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>A character string representing one of the built-in penalties.
<code>"l0"</code> represents the <code class="reqn">L_0</code> penalty, <code>"l1"</code> represents the
lasso-type penalty (<code class="reqn">L_1</code> penalty), and <code>"tlp"</code> represents the
truncated lasso penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>The number of <code>lambda</code> values. Default is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min.ratio</code></td>
<td>
<p>The smallest value for <code>lambda</code>, as a fraction of
<code>lambda.max</code>, the smallest value for which all coefficients are zero.
The default depends on the sample size <code>nobs</code> relative to the number
of variables <code>nvars</code>. If <code>nobs &gt; nvars</code>, the default is
<code>0.0001</code>, and if <code>nobs &lt; nvars</code>, the default is <code>0.01</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>A user-supplied <code>lambda</code> sequence. Typically, users should let
the program compute its own <code>lambda</code> sequence based on
<code>nlambda</code> and <code>lambda.min.ratio</code>. Supplying a value of
<code>lambda</code> will override this. WARNING: please use this option with care.
<code>glmtlp</code> relies on warms starts for speed, and it's often faster to
fit a whole path than a single fit. Therefore, provide a decreasing sequence
of <code>lambda</code> values if you want to use this option. Also, when
<code>penalty = 'l0'</code>, it is not recommended for the users to supply
this parameter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kappa</code></td>
<td>
<p>A user-supplied <code>kappa</code> sequence. Typically, users should
let the program compute its own <code>kappa</code> sequence based on <code>nvars</code>
and <code>nobs</code>. This sequence is used when <code>penalty = 'l0'</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>A tuning parameter used in the TLP-penalized regression models.
Default is  <code>0.3 * sqrt(log(nvars)/nobs)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>A tuning parameter used in the coordinate majorization descent
algorithm. See Yang, Y., &amp; Zou, H. (2014) in the reference for more detail.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Tolerance level for all iterative optimization algorithms.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Observation weights. Default is 1 for each observation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty.factor</code></td>
<td>
<p>Separate penalty factors applied to each
coefficient, which allows for differential shrinkage. Default is 1
for all variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>Logical. Whether or not standardize the input matrix
<code>X</code>; default is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dc.maxit</code></td>
<td>
<p>Maximum number of iterations for the DC (Difference of
Convex Functions) programming; default is 20.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cd.maxit</code></td>
<td>
<p>Maximum number of iterations for the coordinate descent
algorithm; default is 10^4.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nr.maxit</code></td>
<td>
<p>Maximum number of iterations for the Newton-Raphson method;
default is 500.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The sequence of models indexed by <code>lambda</code> (when <code>penalty = c('l1', 'tlp')</code>)
or <code>kappa</code> (when <code>penalty = 'l0'</code>) is fit by the coordinate
descent algorithm.
</p>
<p>The objective function for the <code>"gaussian"</code> family is:
</p>
<p style="text-align: center;"><code class="reqn">1/2 RSS/nobs + \lambda*penalty,</code>
</p>
<p> and for the other models it is:
</p>
<p style="text-align: center;"><code class="reqn">-loglik/nobs + \lambda*penalty.</code>
</p>

<p>Also note that, for <code>"gaussian"</code>, <code>glmtlp</code> standardizes y to
have unit variance (using 1/(n-1) formula).
</p>
<p>## Details on <code>family</code> option
</p>
<p><code>glmtlp</code> currently only supports built-in families, which are specified by a
character string. For all families, the returned object is a regularization
path for fitting the generalized linear regression models, by maximizing the
corresponding penalized log-likelihood. <code>glmtlp(..., family="binomial")</code>
fits a traditional logistic regression model for the log-odds.
</p>
<p>## Details on <code>penalty</code> option
</p>
<p>The built-in penalties are specified by a character string. For <code>l0</code>
penalty, <code>kappa</code> sequence is used for generating the regularization
path, while for <code>l1</code> and <code>tlp</code> penalty, <code>lambda</code> sequence
is used for generating the regularization path.
</p>


<h3>Value</h3>

<p>An object with S3 class <code>"glmtlp"</code>.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>a <code>nvars x length(kappa)</code> matrix of
coefficients when <code>penalty = 'l0'</code>; or a <code>nvars x length(lambda)</code>
matrix of coefficients when <code>penalty = c('l1', 'tlp')</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the call that produces this object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>the distribution family used in the model fitting.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>the intercept vector, of <code>length(kappa)</code> when
<code>penalty = 'l0'</code> or <code>length(lambda)</code> when
<code>penalty = c('l1', 'tlp')</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>the actual sequence of <code>lambda</code> values used. Note that
the length may be smaller than the provided <code>nlambda</code> due to removal
of saturated values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>the penalty type in the model fitting.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty.factor</code></td>
<td>
<p>the penalty factor for each coefficient used in the model fitting.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>the tuning parameter used in the model fitting, available when
<code>penalty = 'tlp'</code>.</p>
</td>
</tr>
</table>
<h3>glmtlp functions</h3>

<p>'glmtlp()', 'cv.glmtlp()'
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Yu Yang <a href="mailto:yuyang.stat@gmail.com">yuyang.stat@gmail.com</a> (<a href="https://orcid.org/0000-0001-7355-6702">ORCID</a>) [copyright holder]
</p>
<p>Authors:
</p>

<ul>
<li>
<p> Chunlin Li <a href="mailto:chunlin@iastate.edu">chunlin@iastate.edu</a> (<a href="https://orcid.org/0000-0003-2989-8785">ORCID</a>) [copyright holder]
</p>
</li>
<li>
<p> Chong Wu (<a href="https://orcid.org/0000-0002-8400-1785">ORCID</a>) [copyright holder]
</p>
</li>
</ul>
<p>Other contributors:
</p>

<ul>
<li>
<p> Xiaotong Shen [thesis advisor, copyright holder]
</p>
</li>
<li>
<p> Wei Pan [thesis advisor, copyright holder]
</p>
</li>
</ul>
<p>Chunlin Li, Yu Yang, Chong Wu
<br> Maintainer: Yu Yang <a href="mailto:yang6367@umn.edu">yang6367@umn.edu</a>
</p>


<h3>References</h3>

<p>Shen, X., Pan, W., &amp; Zhu, Y. (2012).
<em>Likelihood-based selection and sharp parameter estimation.
Journal of the American Statistical Association, 107(497), 223-232.</em>
<br> Shen, X., Pan, W., Zhu, Y., &amp; Zhou, H. (2013).
<em>On constrained and regularized high-dimensional regression.
Annals of the Institute of Statistical Mathematics, 65(5), 807-832.</em>
<br> Li, C., Shen, X., &amp; Pan, W. (2021).
<em>Inference for a Large Directed Graphical Model with Interventions.
arXiv preprint arXiv:2110.03805.</em>
<br> Yang, Y., &amp; Zou, H. (2014).
<em>A coordinate majorization descent algorithm for l1 penalized learning.
Journal of Statistical Computation and Simulation, 84(1), 84-95.</em>
<br> Two R package Github: <em>ncvreg</em> and <em>glmnet</em>.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul><li> <p><a href="https://yuyangyy.com/glmtlp/">https://yuyangyy.com/glmtlp/</a>
</p>
</li></ul>
<p><code>print</code>, <code>predict</code>, <code>coef</code> and <code>plot</code> methods,
and the <code>cv.glmtlp</code> function.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Gaussian
X &lt;- matrix(rnorm(100 * 20), 100, 20)
y &lt;- rnorm(100)
fit1 &lt;- glmtlp(X, y, family = "gaussian", penalty = "l0")
fit2 &lt;- glmtlp(X, y, family = "gaussian", penalty = "l1")
fit3 &lt;- glmtlp(X, y, family = "gaussian", penalty = "tlp")

# Binomial

X &lt;- matrix(rnorm(100 * 20), 100, 20)
y &lt;- sample(c(0, 1), 100, replace = TRUE)
fit &lt;- glmtlp(X, y, family = "binomial", penalty = "l1")
</code></pre>


</div>