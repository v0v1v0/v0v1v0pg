<div class="container">

<table style="width: 100%;"><tr>
<td>summary.gbm</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Summary of a gbm object</h2>

<h3>Description</h3>

<p>Computes the relative influence of each variable in the gbm object.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'gbm'
summary(
  object,
  cBars = length(object$var.names),
  n.trees = object$n.trees,
  plotit = TRUE,
  order = TRUE,
  method = relative.influence,
  normalize = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>a <code>gbm</code> object created from an initial call to
<code>gbm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cBars</code></td>
<td>
<p>the number of bars to plot. If <code>order=TRUE</code> the only the
variables with the <code>cBars</code> largest relative influence will appear in
the barplot. If <code>order=FALSE</code> then the first <code>cBars</code> variables
will appear in the plot. In either case, the function will return the
relative influence of all of the variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.trees</code></td>
<td>
<p>the number of trees used to generate the plot. Only the first
<code>n.trees</code> trees will be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plotit</code></td>
<td>
<p>an indicator as to whether the plot is generated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>order</code></td>
<td>
<p>an indicator as to whether the plotted and/or returned relative
influences are sorted.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>The function used to compute the relative influence.
<code>relative.influence</code> is the default and is the same as that
described in Friedman (2001). The other current (and experimental) choice is
<code>permutation.test.gbm</code>. This method randomly permutes each
predictor variable at a time and computes the associated reduction in
predictive performance. This is similar to the variable importance measures
Breiman uses for random forests, but <code>gbm</code> currently computes using the
entire training dataset (not the out-of-bag observations).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>if <code>FALSE</code> then <code>summary.gbm</code> returns the
unnormalized influence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>other arguments passed to the plot function.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For <code>distribution="gaussian"</code> this returns exactly the reduction of
squared error attributable to each variable. For other loss functions this
returns the reduction attributable to each variable in sum of squared error
in predicting the gradient on each iteration. It describes the relative
influence of each variable in reducing the loss function. See the references
below for exact details on the computation.
</p>


<h3>Value</h3>

<p>Returns a data frame where the first component is the variable name
and the second is the computed relative influence, normalized to sum to 100.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>References</h3>

<p>J.H. Friedman (2001). "Greedy Function Approximation: A Gradient
Boosting Machine," Annals of Statistics 29(5):1189-1232.
</p>
<p>L. Breiman
(2001).<a href="https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf">https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf</a>.
</p>


<h3>See Also</h3>

<p><code>gbm</code>
</p>


</div>