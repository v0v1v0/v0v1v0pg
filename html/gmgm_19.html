<div class="container">

<table style="width: 100%;"><tr>
<td>em</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Estimate the parameters of a Gaussian mixture model</h2>

<h3>Description</h3>

<p>This function estimates the parameters of a Gaussian mixture model using the
expectation-maximization (EM) algorithm. Given an initial model, this
algorithm iteratively updates the parameters, monotonically increasing the
log-likelihood until convergence to a local maximum (Bilmes, 1998). A
Bayesian regularization is applied by default to prevent that a mixture
component comes down to a single point and leads to a zero covariance matrix
(Ormoneit and Tresp, 1996). Although the EM algorithm only applies to the
joint model, good parameters can be found for a derived conditional model.
However, care should be taken as the monotonic increase of the conditional
log-likelihood is not guaranteed.
</p>


<h3>Usage</h3>

<pre><code class="language-R">em(
  gmm,
  data,
  regul = 0.01,
  epsilon = 1e-06,
  max_iter_em = 100,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>gmm</code></td>
<td>
<p>An initial object of class <code>gmm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>A data frame or numeric matrix containing the data used in the
EM algorithm. Its columns must explicitly be named after the variables of
<code>gmm</code> and must not contain missing values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>regul</code></td>
<td>
<p>A positive numeric value corresponding to the regularization
constant if a Bayesian regularization is applied. If <code>NULL</code>, no
regularization is applied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>A positive numeric value corresponding to the convergence
threshold for the increase in log-likelihood.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter_em</code></td>
<td>
<p>A non-negative integer corresponding to the maximum number
of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>A logical value indicating whether iterations in progress
are displayed.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list with elements:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>gmm</code></td>
<td>
<p>The final <code>gmm</code> object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>posterior</code></td>
<td>
<p>A numeric matrix containing the posterior probabilities for
each observation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seq_loglik</code></td>
<td>
<p>A numeric vector containing the sequence of log-likelihoods
measured initially and after each iteration.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Bilmes, J. A. (1998). A Gentle Tutorial of the EM Algorithm and its
Application to Parameter Estimation for Gaussian Mixture and Hidden Markov
Models. Technical report, International Computer Science Institute.
</p>
<p>Ormoneit, D. and Tresp, V. (1996). Improved Gaussian Mixture Density
Estimates Using Bayesian Penalty Terms and Network Averaging. In
<em>Advances in Neural Information Processing Systems 8</em>, pages 542â€“548.
</p>


<h3>See Also</h3>

<p><code>smem</code>, <code>stepwise</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(data_body)
gmm_1 &lt;- split_comp(add_var(NULL,
                            data_body[, c("WAIST", "AGE", "FAT", "HEIGHT",
                                          "WEIGHT")]),
                    n_sub = 3)
res_em &lt;- em(gmm_1, data_body, verbose = TRUE)

</code></pre>


</div>