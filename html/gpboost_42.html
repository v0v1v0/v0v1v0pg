<div class="container">

<table style="width: 100%;"><tr>
<td>gpboost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Train a GPBoost model</h2>

<h3>Description</h3>

<p>Simple interface for training a GPBoost model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gpboost(data, label = NULL, weight = NULL, params = list(),
  nrounds = 100L, gp_model = NULL, line_search_step_length = FALSE,
  use_gp_model_for_validation = TRUE, train_gp_model_cov_pars = TRUE,
  valids = list(), obj = NULL, eval = NULL, verbose = 1L,
  record = TRUE, eval_freq = 1L, early_stopping_rounds = NULL,
  init_model = NULL, colnames = NULL, categorical_feature = NULL,
  callbacks = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>a <code>gpb.Dataset</code> object, used for training. Some functions, such as <code>gpb.cv</code>,
may allow you to pass other types of data like <code>matrix</code> and then separately supply
<code>label</code> as a keyword argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>label</code></td>
<td>
<p>Vector of response values / labels, used if <code>data</code> is not an <code>gpb.Dataset</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight</code></td>
<td>
<p>Vector of weights. The GPBoost algorithm currently does not support weights</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>
<p>list of "tuning" parameters. 
See <a href="https://github.com/fabsig/GPBoost/blob/master/docs/Parameters.rst">the parameter documentation</a> for more information. 
A few key parameters:
</p>

<ul>
<li>
<p><code>learning_rate</code>: The learning rate, also called shrinkage or damping parameter 
(default = 0.1). An important tuning parameter for boosting. Lower values usually 
lead to higher predictive accuracy but more boosting iterations are needed 
</p>
</li>
<li>
<p><code>num_leaves</code>: Number of leaves in a tree. Tuning parameter for 
tree-boosting (default = 31)
</p>
</li>
<li>
<p><code>max_depth</code>: Maximal depth of a tree. Tuning parameter for tree-boosting (default = no limit)
</p>
</li>
<li>
<p><code>min_data_in_leaf</code>: Minimal number of samples per leaf. Tuning parameter for 
tree-boosting (default = 20)
</p>
</li>
<li>
<p><code>lambda_l2</code>: L2 regularization (default = 0)
</p>
</li>
<li>
<p><code>lambda_l1</code>: L1 regularization (default = 0)
</p>
</li>
<li>
<p><code>max_bin</code>: Maximal number of bins that feature values will be bucketed in (default = 255)
</p>
</li>
<li>
<p><code>line_search_step_length</code> (default = FALSE): If TRUE, a line search is done to find the optimal 
step length for every boosting update (see, e.g., Friedman 2001). This is then multiplied by the learning rate 
</p>
</li>
<li>
<p><code>train_gp_model_cov_pars</code> (default = TRUE): If TRUE, the covariance parameters of the Gaussian process 
are estimated in every boosting iterations,  otherwise the gp_model parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide values via 
the 'init_cov_pars' parameter when creating the gp_model 
</p>
</li>
<li>
<p><code>use_gp_model_for_validation</code> (default = TRUE): If TRUE, the Gaussian process is also used 
(in addition to the tree model) for calculating predictions on the validation data 
</p>
</li>
<li>
<p><code>leaves_newton_update</code> (default = FALSE): Set this to TRUE to do a Newton update step for the tree leaves 
after the gradient step. Applies only to Gaussian process boosting (GPBoost algorithm) 
</p>
</li>
<li>
<p>num_threads: Number of threads. For the best speed, set this to
the number of real CPU cores(<code>parallel::detectCores(logical = FALSE)</code>),
not the number of threads (most CPU using hyper-threading to generate 2 threads
per CPU core).
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nrounds</code></td>
<td>
<p>number of boosting iterations (= number of trees). This is the most important tuning parameter for boosting</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gp_model</code></td>
<td>
<p>A <code>GPModel</code> object that contains the random effects (Gaussian process and / or grouped random effects) model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>line_search_step_length</code></td>
<td>
<p>Boolean. If TRUE, a line search is done to find the optimal step length for every boosting update 
(see, e.g., Friedman 2001). This is then multiplied by the <code>learning_rate</code>. 
Applies only to the GPBoost algorithm</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_gp_model_for_validation</code></td>
<td>
<p>Boolean. If TRUE, the <code>gp_model</code> 
(Gaussian process and/or random effects) is also used (in addition to the tree model) for calculating 
predictions on the validation data. If FALSE, the <code>gp_model</code> (random effects part) is ignored 
for making predictions and only the tree ensemble is used for making predictions for calculating the validation / test error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train_gp_model_cov_pars</code></td>
<td>
<p>Boolean. If TRUE, the covariance parameters 
of the <code>gp_model</code> (Gaussian process and/or random effects) are estimated in every 
boosting iterations, otherwise the <code>gp_model</code> parameters are not estimated. 
In the latter case, you need to either estimate them beforehand or provide the values via 
the <code>init_cov_pars</code> parameter when creating the <code>gp_model</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>valids</code></td>
<td>
<p>a list of <code>gpb.Dataset</code> objects, used for validation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obj</code></td>
<td>
<p>(character) The distribution of the response variable (=label) conditional on fixed and random effects.
This only needs to be set when doing independent boosting without random effects / Gaussian processes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eval</code></td>
<td>
<p>Evaluation metric to be monitored when doing CV and parameter tuning. 
This can be a string, function, or list with a mixture of strings and functions.
</p>

<ul>
<li>
<p><b>a. character vector</b>:
Non-exhaustive list of supported metrics: "test_neg_log_likelihood", "mse", "rmse", "mae", 
"auc", "average_precision", "binary_logloss", "binary_error"
See <a href="https://gpboost.readthedocs.io/en/latest/Parameters.html#metric-parameters">
the "metric" section of the parameter documentation</a>
for a complete list of valid metrics.

</p>
</li>
<li>
<p><b>b. function</b>:
You can provide a custom evaluation function. This
should accept the keyword arguments <code>preds</code> and <code>dtrain</code> and should return a named
list with three elements:
</p>

<ul>
<li>
<p><code>name</code>: A string with the name of the metric, used for printing
and storing results.

</p>
</li>
<li>
<p><code>value</code>: A single number indicating the value of the metric for the
given predictions and true values

</p>
</li>
<li>
<p><code>higher_better</code>: A boolean indicating whether higher values indicate a better fit.
For example, this would be <code>FALSE</code> for metrics like MAE or RMSE.

</p>
</li>
</ul>
</li>
<li>
<p><b>c. list</b>:
If a list is given, it should only contain character vectors and functions.
These should follow the requirements from the descriptions above.

</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>verbosity for output, if &lt;= 0, also will disable the print of evaluation during training</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>record</code></td>
<td>
<p>Boolean, TRUE will record iteration message to <code>booster$record_evals</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eval_freq</code></td>
<td>
<p>evaluation output frequency, only effect when verbose &gt; 0</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>early_stopping_rounds</code></td>
<td>
<p>int. Activates early stopping. Requires at least one validation data
and one metric. When this parameter is non-null,
training will stop if the evaluation of any metric on any validation set
fails to improve for <code>early_stopping_rounds</code> consecutive boosting rounds.
If training stops early, the returned model will have attribute <code>best_iter</code>
set to the iteration number of the best iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init_model</code></td>
<td>
<p>path of model file of <code>gpb.Booster</code> object, will continue training from this model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>colnames</code></td>
<td>
<p>feature names, if not null, will use this to overwrite the names in dataset</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>categorical_feature</code></td>
<td>
<p>categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g.
<code>c(1L, 10L)</code> to say "the first and tenth columns").</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>callbacks</code></td>
<td>
<p>List of callback functions that are applied at each iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments passed to <code>gpb.train</code>. For example
</p>

<ul>
<li>
<p><code>valids</code>: a list of <code>gpb.Dataset</code> objects, used for validation
</p>
</li>
<li>
<p><code>eval</code>: evaluation function, can be (a list of) character or custom eval function
</p>
</li>
<li>
<p><code>record</code>: Boolean, TRUE will record iteration message to <code>booster$record_evals</code>
</p>
</li>
<li>
<p><code>colnames</code>: feature names, if not null, will use this to overwrite the names in dataset
</p>
</li>
<li>
<p><code>categorical_feature</code>: categorical features. This can either be a character vector of feature
names or an integer vector with the indices of the features (e.g. <code>c(1L, 10L)</code> to
say "the first and tenth columns").
</p>
</li>
<li>
<p><code>reset_data</code>: Boolean, setting it to TRUE (not the default value) will transform the booster model
into a predictor model which frees up memory and the original datasets
</p>
</li>
</ul>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a trained <code>gpb.Booster</code>
</p>


<h3>Early Stopping</h3>

<p>"early stopping" refers to stopping the training process if the model's performance on a given
validation set does not improve for several consecutive iterations.
</p>
<p>If multiple arguments are given to <code>eval</code>, their order will be preserved. If you enable
early stopping by setting <code>early_stopping_rounds</code> in <code>params</code>, by default all
metrics will be considered for early stopping.
</p>
<p>If you want to only consider the first metric for early stopping, pass
<code>first_metric_only = TRUE</code> in <code>params</code>. Note that if you also specify <code>metric</code>
in <code>params</code>, that metric will be considered the "first" one. If you omit <code>metric</code>,
a default metric will be used based on your choice for the parameter <code>obj</code> (keyword argument)
or <code>objective</code> (passed into <code>params</code>).
</p>


<h3>Author(s)</h3>

<p>Fabio Sigrist, authors of the LightGBM R package
</p>


<h3>Examples</h3>

<pre><code class="language-R"># See https://github.com/fabsig/GPBoost/tree/master/R-package for more examples


library(gpboost)
data(GPBoost_data, package = "gpboost")

#--------------------Combine tree-boosting and grouped random effects model----------------
# Create random effects model
gp_model &lt;- GPModel(group_data = group_data[,1], likelihood = "gaussian")
# The default optimizer for covariance parameters (hyperparameters) is 
# Nesterov-accelerated gradient descent.
# This can be changed to, e.g., Nelder-Mead as follows:
# re_params &lt;- list(optimizer_cov = "nelder_mead")
# gp_model$set_optim_params(params=re_params)
# Use trace = TRUE to monitor convergence:
# re_params &lt;- list(trace = TRUE)
# gp_model$set_optim_params(params=re_params)

# Train model
bst &lt;- gpboost(data = X, label = y, gp_model = gp_model, nrounds = 16,
               learning_rate = 0.05, max_depth = 6, min_data_in_leaf = 5,
               verbose = 0)
# Estimated random effects model
summary(gp_model)

# Make predictions
# Predict latent variables
pred &lt;- predict(bst, data = X_test, group_data_pred = group_data_test[,1],
                predict_var = TRUE, pred_latent = TRUE)
pred$random_effect_mean # Predicted latent random effects mean
pred$random_effect_cov # Predicted random effects variances
pred$fixed_effect # Predicted fixed effects from tree ensemble
# Predict response variable
pred_resp &lt;- predict(bst, data = X_test, group_data_pred = group_data_test[,1],
                     predict_var = TRUE, pred_latent = FALSE)
pred_resp$response_mean # Predicted response mean
# For Gaussian data: pred$random_effect_mean + pred$fixed_effect = pred_resp$response_mean
pred$random_effect_mean + pred$fixed_effect - pred_resp$response_mean

#--------------------Combine tree-boosting and Gaussian process model----------------
# Create Gaussian process model
gp_model &lt;- GPModel(gp_coords = coords, cov_function = "exponential",
                    likelihood = "gaussian")
# Train model
bst &lt;- gpboost(data = X, label = y, gp_model = gp_model, nrounds = 8,
               learning_rate = 0.1, max_depth = 6, min_data_in_leaf = 5,
               verbose = 0)
# Estimated random effects model
summary(gp_model)
# Make predictions
pred &lt;- predict(bst, data = X_test, gp_coords_pred = coords_test,
                predict_var = TRUE, pred_latent = TRUE)
pred$random_effect_mean # Predicted latent random effects mean
pred$random_effect_cov # Predicted random effects variances
pred$fixed_effect # Predicted fixed effects from tree ensemble
# Predict response variable
pred_resp &lt;- predict(bst, data = X_test, gp_coords_pred = coords_test,
                     predict_var = TRUE, pred_latent = FALSE)
pred_resp$response_mean # Predicted response mean

</code></pre>


</div>