<div class="container">

<table style="width: 100%;"><tr>
<td>nn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>A interface function to use  nnet() function within GAMLSS 
</h2>

<h3>Description</h3>

<p>The <code>nn()</code> function is a additive function to be used for GAMLSS models. 
It is an interface for the <code>nnet()</code> function of package 
<code>nnet</code> of Brian Ripley. The function <code>nn()</code> allows the user to use neural networks 
within <code>gamlss</code>. The great advantage of course comes from the fact GAMLSS models provide a variety of distributions and diagnostics.   
</p>


<h3>Usage</h3>

<pre><code class="language-R">nn(formula, control = nn.control(...), ...)
nn.control(size = 3, linout = TRUE, entropy = FALSE, softmax = FALSE, 
           censored = FALSE, skip = FALSE, rang = 0.7, decay = 0, 
           maxit = 100, Hess = FALSE, trace = FALSE, 
           MaxNWts = 1000, abstol = 1e-04, reltol = 1e-08)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p> A formula containing the expolanatory variables i.e. ~x1+x2+x3.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>control to pass the arguments for the nnet() function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>for extra arguments</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>size</code></td>
<td>
<p>number of units in the hidden layer. Can be zero if there are skip-layer units</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>linout</code></td>
<td>
<p>switch for linear output units. Default is TRUE, identily link</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>entropy</code></td>
<td>
<p>switch for entropy (= maximum conditional likelihood) fitting. Default by least-squares.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>softmax</code></td>
<td>
<p>switch for softmax (log-linear model) and maximum conditional likelihood fitting. linout, entropy, softmax and censored are mutually exclusive.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>censored</code></td>
<td>
<p>A variant on softmax, in which non-zero targets mean possible classes. Thus for softmax a row of (0, 1, 1) means one example each of classes 2 and 3, but for censored it means one example whose class is only known to be 2 or 3.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>skip</code></td>
<td>
<p>switch to add skip-layer connections from input to output</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rang</code></td>
<td>
<p>Initial random weights on <code>[-rang, rang]</code>. Value about 0.5 unless the inputs are large, in which case it should be chosen so that <code>rang * max(|x|)</code> is about 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>decay</code></td>
<td>
<p>parameter for weight decay. Default 0. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>parameter for weight decay. Default 0. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Hess</code></td>
<td>
<p>If true, the Hessian of the measure of fit at the best set of weights found is returned as component Hessian. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>switch for tracing optimization. Default FALSE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MaxNWts</code></td>
<td>
<p>The maximum allowable number of weights. There is no intrinsic limit in the code, but increasing MaxNWts will probably allow fits that are very slow and time-consuming. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>abstol</code></td>
<td>
<p>Stop if the fit criterion falls below abstol, indicating an essentially perfect fit. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reltol</code></td>
<td>
<p>Stop if the optimizer is unable to reduce the fit criterion by a factor of at least 1 - reltol.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Note that, neural networks are over parameterized models and therefor notorious for multiple maximum.  
There is no guarantee that two identical fits will produce identical results.</p>


<h3>Value</h3>

<p>Note that <code>nn</code> itself does no smoothing; it simply sets things up for the function <code>gamlss()</code> which in turn uses the function 
<code>additive.fit()</code> for backfitting which in turn uses <code>gamlss.nn()</code></p>


<h3>Warning </h3>

<p>You may have to fit the model several time to unsure that you obtain a reasonable minimum</p>


<h3>Author(s)</h3>

<p>Mikis Stasinopoulos <a href="mailto:d.stasinopoulos@londonmet.ac.uk">d.stasinopoulos@londonmet.ac.uk</a>, Bob Rigby 
based on work of  Venables &amp; Ripley wich also based on work by Kurt Hornik and Albrecht Gebhardt.
</p>


<h3>References</h3>

<p>Rigby, R. A. and  Stasinopoulos D. M. (2005). Generalized additive models for location, scale and shape,(with discussion), 
<em>Appl. Statist.</em>, <b>54</b>, part 3, pp 507-554.
</p>
<p>Rigby R.A., Stasinopoulos D. M., Heller G., and De Bastiani F., (2019) <em>Distributions for Modeling Location, Scale and Shape: Using GAMLSS in R</em>, Chapman and Hall/CRC.
</p>
<p>Ripley, B. D. (1996) <em>Pattern Recognition and Neural Networks</em>. Cambridge. 
</p>
<p>Stasinopoulos D. M. Rigby R.A. (2007) Generalized additive models for location scale and shape (GAMLSS) in R.
<em>Journal of Statistical Software</em>, <b>23</b>(7), 1â€“46, <a href="https://doi.org/10.18637/jss.v023.i07">doi:10.18637/jss.v023.i07</a>
</p>
<p>Stasinopoulos D. M., Rigby R.A., Heller G., Voudouris V., and De Bastiani F., (2017) <em>Flexible Regression and Smoothing: Using GAMLSS in R</em>, Chapman and Hall/CRC. 
(see also <a href="https://www.gamlss.com/">https://www.gamlss.com/</a>).
</p>
<p>Venables, W. N. and Ripley, B. D. (2002) <em>Modern Applied Statistics with S</em>. Fourth edition. Springer. 
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(nnet)
data(rock)
area1&lt;- with(rock,area/10000)
peri1&lt;- with (rock,peri/10000) 
rock1&lt;- with(rock, data.frame(perm, area=area1, peri=peri1, shape))
# fit nnet
r1 &lt;- nnet(log(perm)~area+peri+shape, rock1, size=3, decay=1e-3, linout=TRUE, 
            skip=TRUE, max=1000, Hess=TRUE)
summary(r1) 
# get gamlss
library(gamlss) 
cc &lt;- nn.control(size=3, decay=1e-3, linout=TRUE, skip=TRUE, max=1000, 
      Hess=TRUE)
g1 &lt;- gamlss(log(perm)~nn(~area+peri+shape,size=3, control=cc), data=rock1)
summary(g1$mu.coefSmo[[1]])
# predict
Xp &lt;- expand.grid(area=seq(0.1,1.2,0.05), peri=seq(0,0.5, 0.02), shape=0.2)
rocknew &lt;- cbind(Xp, fit=predict(r1, newdata=Xp))
library(lattice)
wf1&lt;-wireframe(fit~area+peri, rocknew, screen=list(z=160, x=-60), 
               aspect=c(1, 0.5), drape=TRUE,  main="nnet()")
rocknew1 &lt;- cbind(Xp, fit=predict(g1, newdata=Xp))
wf2&lt;-wireframe(fit~area+peri, rocknew1, screen=list(z=160, x=-60), 
               aspect=c(1, 0.5), drape=TRUE,  main="nn()")
print(wf1, split=c(1,1,2,1), more=TRUE)
print(wf2, split=c(2,1,2,1))
#------------------------------------------------------------------------
 data(rent)
 mr1 &lt;- gamlss(R~nn(~Fl+A, size=5, decay=0.001), data=rent, family=GA)  
 library(gamlss.add)
 mg1&lt;-gamlss(R~ga(~s(Fl,A)), data=rent, family=GA) 
 AIC(mr1,mg1)
newrent &lt;- newrent1 &lt;-newrent2 &lt;- data.frame(expand.grid(Fl=seq(30,120,5),
                   A=seq(1890,1990,5 )))
newrent1$fit &lt;- predict(mr1, newdata=newrent, type="response") ##nn
newrent2$fit &lt;- predict(mg1, newdata=newrent, type="response")# gam
 library(lattice)
 wf1&lt;-wireframe(fit~Fl+A, newrent1, aspect=c(1,0.5), drape=TRUE, 
                colorkey=(list(space="right", height=0.6)), main="nn()")
 wf2&lt;-wireframe(fit~Fl+A, newrent2, aspect=c(1,0.5), drape=TRUE, 
                colorkey=(list(space="right", height=0.6)), main="ga()")
print(wf1, split=c(1,1,2,1), more=TRUE)
print(wf2, split=c(2,1,2,1))
#-------------------------------------------------------------------------
## Not run: 
data(db)
mdb1 &lt;- gamlss(head~nn(~age,size=20, decay=0.001), data=db)
plot(head~age, data=db)
points(fitted(mdb1)~db$age, col="red")
mdb2 &lt;- gamlss(head~nn(~age,size=20, decay=0.001), data=db, family=BCT)
plot(head~age, data=db)
points(fitted(mdb2)~db$age, col="red")

## End(Not run)
</code></pre>


</div>