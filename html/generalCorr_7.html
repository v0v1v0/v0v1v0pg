<div class="container">

<table style="width: 100%;"><tr>
<td>abs_stdapdC</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Absolute values of gradients (apd's) of kernel regressions of x on y when
both x and y are standardized and control variables are present.</h2>

<h3>Description</h3>

<p>1) standardize the data to force mean zero and variance unity, 2) kernel
regress x on y and a matrix of control variables, 
with the option ‘gradients = TRUE’ and finally 3) compute
the absolute values of gradients
</p>


<h3>Usage</h3>

<pre><code class="language-R">abs_stdapdC(x, y, ctrl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>vector of data on the dependent variable</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>data on the regressors which can be a matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ctrl</code></td>
<td>
<p>Data matrix on the control variable(s) beyond causal path issues</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The first argument is assumed to be the dependent variable.  If
<code>abs_stdapdC(x,y)</code> is used, you are regressing x on y (not the usual y
on x). The regressors can be a matrix with 2 or more columns. The missing values
are suitably ignored by the standardization.
</p>


<h3>Value</h3>

<p>Absolute values of kernel regression gradients are returned after
standardizing the data on both sides so that the magnitudes of amorphous
partial derivatives (apd's) are comparable between regression of x on y on
the one hand and regression of y on x on the other.
</p>


<h3>Author(s)</h3>

<p>Prof. H. D. Vinod, Economics Dept., Fordham University, NY
</p>


<h3>See Also</h3>

<p>See  <code>abs_stdapd</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
set.seed(330)
x=sample(20:50)
y=sample(20:50)
z=sample(20:50)
abs_stdapdC(x,y,ctrl=z)

## End(Not run)
</code></pre>


</div>