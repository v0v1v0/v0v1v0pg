<div class="container">

<table style="width: 100%;"><tr>
<td>evaluate_test_metrics</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Evaluate Test Metrics for a Grid Search Model</h2>

<h3>Description</h3>

<p>This function takes a grid search object, test data, and test labels to evaluate the performance
of the best model found during grid search.
</p>


<h3>Usage</h3>

<pre><code class="language-R">evaluate_test_metrics(grid_search, X_test, y_test, modules)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>grid_search</code></td>
<td>
<p>A grid search object containing the best estimator.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X_test</code></td>
<td>
<p>A data frame or matrix of test features.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y_test</code></td>
<td>
<p>A vector of test labels.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>modules</code></td>
<td>
<p>A list of Python modules used in the function.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list containing key performance metrics of the best model:
- @field precision: The weighted precision score.
- @field recall: The weighted recall score.
- @field f1: The weighted F1 score.
- @field accuracy: The overall accuracy score.
These metrics are crucial for evaluating the effectiveness of the model on test data.
</p>


</div>