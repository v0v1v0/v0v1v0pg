<div class="container">

<table style="width: 100%;"><tr>
<td>test_calibration</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Omnibus evaluation of the quality of the random forest estimates via calibration.</h2>

<h3>Description</h3>

<p>Test calibration of the forest. Computes the best linear fit of the target
estimand using the forest prediction (on held-out data) as well as the mean
forest prediction as the sole two regressors. A coefficient of 1 for
'mean.forest.prediction' suggests that the mean forest prediction is correct,
whereas a coefficient of 1 for 'differential.forest.prediction' additionally suggests
that the heterogeneity estimates from the forest are well calibrated.
The p-value of the 'differential.forest.prediction' coefficient
also acts as an omnibus test for the presence of heterogeneity: If the coefficient
is significantly greater than 0, then we can reject the null of
no heterogeneity. For another class of omnnibus tests see <code>rank_average_treatment_effect</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">test_calibration(forest, vcov.type = "HC3")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>forest</code></td>
<td>
<p>The trained forest.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vcov.type</code></td>
<td>
<p>Optional covariance type for standard errors. The possible
options are HC0, ..., HC3. The default is "HC3", which is recommended in small
samples and corresponds to the "shortcut formula" for the jackknife
(see MacKinnon &amp; White for more discussion, and Cameron &amp; Miller for a review).
For large data sets with clusters, "HC0" or "HC1" are significantly faster to compute.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A heteroskedasticity-consistent test of calibration.
</p>


<h3>References</h3>

<p>Cameron, A. Colin, and Douglas L. Miller. "A practitioner's guide to
cluster-robust inference." Journal of Human Resources 50, no. 2 (2015): 317-372.
</p>
<p>Chernozhukov, Victor, Mert Demirer, Esther Duflo, and Ivan Fernandez-Val.
"Generic Machine Learning Inference on Heterogenous Treatment Effects in
Randomized Experiments." arXiv preprint arXiv:1712.04802 (2017).
</p>
<p>MacKinnon, James G., and Halbert White. "Some heteroskedasticity-consistent
covariance matrix estimators with improved finite sample properties."
Journal of Econometrics 29.3 (1985): 305-325.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
n &lt;- 800
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.25 + 0.5 * (X[, 1] &gt; 0))
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
forest &lt;- causal_forest(X, Y, W)
test_calibration(forest)


</code></pre>


</div>