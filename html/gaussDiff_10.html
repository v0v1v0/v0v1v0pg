<div class="container">

<table style="width: 100%;"><tr>
<td>normdiff</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Difference measures for multivariate Gaussian pdfs</h2>

<h3>Description</h3>

<p>Various difference measures for Gaussian pdfs are
implemented: Euclidean distance of the means, Mahalanobis
distance, Kullback-Leibler divergence, J-Coefficient, 
Minkowski L2-distance, Chi-square divergence and the Hellinger
coefficient which is a similarity measure.
</p>


<h3>Usage</h3>

<pre><code class="language-R">normdiff(mu1,sigma1=NULL,mu2,sigma2=sigma1,inv=FALSE,s=0.5,
method=c("Mahalanobis","KL","J","Chisq",
"Hellinger","L2","Euclidean"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>mu1</code></td>
<td>
<p>mean value of pdf 1, a vector</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma1</code></td>
<td>
<p>covariance matrix of pdf 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu2</code></td>
<td>
<p>mean value of pdf 2, a vector</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma2</code></td>
<td>
<p>covariance matrix of pdf 2</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>difference measure to be used, see below</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>inv</code></td>
<td>
<p>if TRUE, 1-Hellinger is reported, default: <code>inv=FALSE</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s</code></td>
<td>
<p>exponent for Hellinger coefficient, default: <code>s=0.5</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Equations can be found in H.-H. Bock, <cite>Analysis of
Symbolic Data</cite>, Chapter <cite>Dissimilarity Measures for Probability
Distributions</cite>
</p>


<h3>Value</h3>

<p>A scalar object of class <code>normdiff</code> reporting the distance.
</p>


<h3>Author(s)</h3>

<p>Henning Rust, <a href="mailto:henning.rust@met.fu-berlin.de">henning.rust@met.fu-berlin.de</a>
</p>


<h3>References</h3>

<p>H.-H. Bock, <cite>Analysis of Symbolic Data</cite>, Chapter
<cite>Dissimilarity measures for Probabilistic Distributions</cite>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(gaussDiff)
mu1 &lt;- c(0,0,0)
sig1 &lt;- diag(c(1,1,1))
mu2 &lt;- c(1,1,1)
sig2 &lt;- diag(c(0.5,0.5,0.5))

## Euclidean distance
normdiff(mu1=mu1,mu2=mu2,method="Euclidean")

## Mahalanobis distance
normdiff(mu1=mu1,sigma1=sig1,mu2=mu2,method="Mahalanobis")

## Kullback-Leibler divergence
normdiff(mu1=mu1,sigma1=sig1,mu2=mu2,sigma2=sig2,method="KL")

## J-Coefficient
normdiff(mu1=mu1,sigma1=sig1,mu2=mu2,sigma2=sig2,method="J")

## Chi-sqr divergence
normdiff(mu1=mu1,sigma1=sig1,mu2=mu2,sigma2=sig2,method="Chisq")

## Minkowsi L2 distance
normdiff(mu1=mu1,sigma1=sig1,mu2=mu2,sigma2=sig2,method="L2")

## Hellinger coefficient
normdiff(mu1=mu1,sigma1=sig1,mu2=mu2,sigma2=sig2,method="Hellinger")
</code></pre>


</div>