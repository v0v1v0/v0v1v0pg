<div class="container">

<table style="width: 100%;"><tr>
<td>FNN</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generative Moment Matching Network</h2>

<h3>Description</h3>

<p>Constructor for a generative feedforward neural network (FNN) model,
an object of <code>S3</code> class <code>"gnn_FNN"</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">FNN(dim = c(2, 2), activation = c(rep("relu", length(dim) - 2), "sigmoid"),
    batch.norm = FALSE, dropout.rate = 0, loss.fun = "MMD", n.GPU = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p><code>integer</code> vector of length at least two, giving
the dimensions of the input layer, the hidden layer(s) (if any) and
the output layer (in this order).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p><code>character</code> vector of length
<code>length(dim) - 1</code> specifying the activation functions
for all hidden layers and the output layer (in this order);
note that the input layer does not have an activation function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss.fun</code></td>
<td>
<p>loss function specified as <code>character</code>
or <code>function</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch.norm</code></td>
<td>
<p><code>logical</code> indicating whether batch
normalization layers are to be added after each hidden layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout.rate</code></td>
<td>
<p><code>numeric</code> value in [0,1] specifying
the fraction of input to be dropped; see the rate parameter of
<code>layer_dropout()</code>. Note that only if positive, dropout
layers are added after each hidden layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.GPU</code></td>
<td>
<p>non-negative <code>integer</code> specifying the number of GPUs
available if the GPU version of TensorFlow is installed.
If positive, a (special) multiple GPU model for data
parallelism is instantiated. Note that for multi-layer perceptrons
on a few GPUs, this model does not yet yield any scale-up computational
factor (in fact, currently very slightly negative scale-ups are likely due
to overhead costs).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments passed to <code>loss()</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>S3</code> class <code>"gnn_FNN"</code> is a subclass of the
<code>S3</code> class <code>"gnn_GNN"</code> which in turn is a subclass of
<code>"gnn_Model"</code>.
</p>


<h3>Value</h3>

<p><code>FNN()</code> returns an object of <code>S3</code> class <code>"gnn_FNN"</code>
with components
</p>

<dl>
<dt><code>model</code></dt>
<dd>
<p>FNN model (a <span class="pkg">keras</span> object inheriting from
the R6 classes <code>"keras.engine.training.Model"</code>,
<code>"keras.engine.network.Network"</code>,
<code>"keras.engine.base_layer.Layer"</code>
and <code>"python.builtin.object"</code>, or a <code>raw</code>
object).</p>
</dd>
<dt><code>type</code></dt>
<dd>
<p><code>character</code> string indicating
the type of model.</p>
</dd>
<dt><code>dim</code></dt>
<dd>
<p>see above.</p>
</dd>
<dt><code>activation</code></dt>
<dd>
<p>see above.</p>
</dd>
<dt><code>batch.norm</code></dt>
<dd>
<p>see above.</p>
</dd>
<dt><code>dropout.rate</code></dt>
<dd>
<p>see above.</p>
</dd>
<dt><code>n.param</code></dt>
<dd>
<p>number of trainable, non-trainable and total
number of parameters.</p>
</dd>
<dt><code>loss.type</code></dt>
<dd>
<p>type of loss function (<code>character</code>).</p>
</dd>
<dt><code>n.train</code></dt>
<dd>
<p>number of training samples (<code>NA_integer_</code>
unless trained).</p>
</dd>
<dt><code>batch.size</code></dt>
<dd>
<p>batch size (<code>NA_integer_</code> unless trained).</p>
</dd>
<dt><code>n.epoch</code></dt>
<dd>
<p>number of epochs (<code>NA_integer_</code>
unless trained).</p>
</dd>
<dt><code>loss</code></dt>
<dd>
<p><code>numeric(n.epoch)</code> containing the
loss function values per epoch.</p>
</dd>
<dt><code>time</code></dt>
<dd>
<p>object of S3 class <code>"proc_time"</code>
containing the training time (if trained).</p>
</dd>
<dt><code>prior</code></dt>
<dd>
<p><code>matrix</code> containing a (sub-)sample
of the prior (if trained).</p>
</dd>
</dl>
<h3>Author(s)</h3>

<p>Marius Hofert and Avinash Prasad</p>


<h3>References</h3>

<p>Li, Y., Swersky, K. and Zemel, R. (2015).
Generative moment matching networks.
<em>Proceedings of Machine Learning Research</em>, <b>37</b>
(International Conference on Maching Learning), 1718–1727.
See http://proceedings.mlr.press/v37/li15.pdf (2019-08-24)
</p>
<p>Dziugaite, G. K., Roy, D. M. and Ghahramani, Z. (2015).
Training generative neural networks via maximum mean discrepancy
optimization. <em>AUAI Press</em>, 258–267.
See http://www.auai.org/uai2015/proceedings/papers/230.pdf (2019-08-24)
</p>
<p>Hofert, M., Prasad, A. and Zhu, M. (2020).
Quasi-random sampling for multivariate distributions via generative
neural networks. <em>Journal of Computational and Graphical
Statistics</em>, <a href="https://doi.org/10.1080/10618600.2020.1868302">doi:10.1080/10618600.2020.1868302</a>.
</p>
<p>Hofert, M., Prasad, A. and Zhu, M. (2020).
Multivariate time-series modeling with generative neural networks.
See <a href="https://arxiv.org/abs/2002.10645">https://arxiv.org/abs/2002.10645</a>.
</p>
<p>Hofert, M. Prasad, A. and Zhu, M. (2020).
Applications of multivariate quasi-random sampling with neural
networks. See <a href="https://arxiv.org/abs/2012.08036">https://arxiv.org/abs/2012.08036</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">if(TensorFlow_available()) { # rather restrictive (due to R-Forge, winbuilder)
library(gnn) # for being standalone

## Training data
d &lt;- 2 # bivariate case
P &lt;- matrix(0.9, nrow = d, ncol = d); diag(P) &lt;- 1 # correlation matrix
ntrn &lt;- 60000 # training data sample size
set.seed(271)
library(nvmix)
X &lt;- abs(rNorm(ntrn, scale = P)) # componentwise absolute values of N(0,P) sample

## Plot a subsample
m &lt;- 2000 # subsample size for plots
opar &lt;- par(pty = "s")
plot(X[1:m,], xlab = expression(X[1]), ylab = expression(X[2])) # plot |X|
U &lt;- apply(X, 2, rank) / (ntrn + 1) # pseudo-observations of |X|
plot(U[1:m,], xlab = expression(U[1]), ylab = expression(U[2])) # visual check

## Model 1: A basic feedforward neural network (FNN) with MSE loss function
fnn &lt;- FNN(c(d, 300, d), loss.fun = "MSE") # define the FNN
fnn &lt;- fitGNN(fnn, data = U, n.epoch = 40) # train with batch optimization
plot(fnn, kind = "loss") # plot the loss after each epoch

## Model 2: A GMMN (FNN with MMD loss function)
gmmn &lt;- FNN(c(d, 300, d)) # define the GMMN (initialized with random weights)
## For training we need to use a mini-batch optimization (batch size &lt; nrow(U)).
## For a fair comparison (same number of gradient steps) to NN, we use 500
## samples (25% = 4 gradient steps/epoch) for 10 epochs for GMMN.
library(keras) # for callback_early_stopping()
## We monitor the loss function and stop earlier if the loss function
## over the last patience-many epochs has changed by less than min_delta
## in absolute value. Then we keep the weights that led to the smallest
## loss seen throughout training.
gmmn &lt;- fitGNN(gmmn, data = U, batch.size = 500, n.epoch = 10,
               callbacks = callback_early_stopping(monitor = "loss",
                                                   min_delta = 1e-3, patience = 3,
                                                   restore_best_weights = TRUE))
plot(gmmn, kind = "loss") # plot the loss after each epoch
## Note:
## - Obviously, in a real-world application, batch.size and n.epoch
##   should be (much) larger (e.g., batch.size = 5000, n.epoch = 300).
## - Training is not reproducible (due to keras).

## Model 3: A FNN with CvM loss function
fnnCvM &lt;- FNN(c(d, 300, d), loss.fun = "CvM")
fnnCvM &lt;- fitGNN(fnnCvM, data = U, batch.size = 500, n.epoch = 10,
                 callbacks = callback_early_stopping(monitor = "loss",
                                                     min_delta = 1e-3, patience = 3,
                                                     restore_best_weights = TRUE))
plot(fnnCvM, kind = "loss") # plot the loss after each epoch

## Sample from the different models
set.seed(271)
V.fnn &lt;- rGNN(fnn, size = m)
set.seed(271)
V.gmmn &lt;- rGNN(gmmn, size = m)
set.seed(271)
V.fnnCvM &lt;- rGNN(fnnCvM, size = m)

## Joint plot of training subsample with GMMN PRNs. Clearly, the MSE
## cannot be used to learn the distribution correctly.
layout(matrix(1:4, ncol = 2, byrow = TRUE))
plot(U[1:m,], xlab = expression(U[1]), ylab = expression(U[2]), cex = 0.2)
mtext("Training subsample", side = 4, line = 0.4, adj = 0)
plot(V.fnn,    xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with MSE loss", side = 4, line = 0.4, adj = 0)
plot(V.gmmn,  xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with MMD loss", side = 4, line = 0.4, adj = 0)
plot(V.fnnCvM,    xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with CvM loss", side = 4, line = 0.4, adj = 0)

## Joint plot of training subsample with GMMN QRNs
library(qrng) # for sobol()
V.fnn.    &lt;- rGNN(fnn,    size = m, method = "sobol", randomize = "Owen")
V.gmmn.   &lt;- rGNN(gmmn,   size = m, method = "sobol", randomize = "Owen")
V.fnnCvM. &lt;- rGNN(fnnCvM, size = m, method = "sobol", randomize = "Owen")
plot(U[1:m,], xlab = expression(U[1]), ylab = expression(U[2]), cex = 0.2)
mtext("Training subsample", side = 4, line = 0.4, adj = 0)
plot(V.fnn., xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with MSE loss", side = 4, line = 0.4, adj = 0)
plot(V.gmmn., xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with MMD loss", side = 4, line = 0.4, adj = 0)
plot(V.fnnCvM., xlab = expression(V[1]), ylab = expression(V[2]), cex = 0.2)
mtext("Trained NN with CvM loss", side = 4, line = 0.4, adj = 0)
layout(1)
par(opar)
}
</code></pre>


</div>