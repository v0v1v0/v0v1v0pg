<div class="container">

<table style="width: 100%;"><tr>
<td>LR_GradientConjugate_gpumatrix</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Logistic Regression with Conjugate Gradient method</h2>

<h3>Description</h3>

<p>The developed function performs the logistic regression using the Conjugate Gradient method. This method has shown to be very effective for logistic regression of big models  [1]. The code is general enough to accommodate standard R matrices, sparse matrices from the 'Matrix' package and, more interestingly, gpu.matrix-class objects from the GPUmatrix package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">LR_GradientConjugate_gpumatrix(X, y, beta = NULL,
                               lambda = 0, iterations = 100,
                               tol = 1e-08)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>the design matrix. Could be either a object of class <code>gpu.matrix</code>, <code>matrix</code>, or  <code>Matrix</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>initial solution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>numeric. Penalty factor also known as the L2 norm or L2 penalty, which is computed as the sum of the squared coefficients: <code class="reqn">\lambda||\beta_i||_2^2</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iterations</code></td>
<td>
<p>maximum number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>tolerance to be used for the estimation.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If the input gpu.matrix-class object(s) are stored on the GPU, then the operations will be performed on the GPU. See <code>gpu.matrix</code>.
</p>


<h3>Value</h3>

<p>The function returns a vector containing the values of the coefficients. This returned vector will be a 'matrix', 'Matrix' or 'gpu.matrix-class' object depending on the class of the object <code>X</code>.
</p>


<h3>Author(s)</h3>

<p>Angel Rubio and Cesar Lobato.
</p>


<h3>References</h3>

<p>[1] Minka TP (2003). “A comparison of numerical optimizers for logistic regression.” URL: https://tminka.github.io/papers/logreg/minka-logreg.pdf.
</p>


<h3>See Also</h3>

<p>See also:  <code>GPUglm</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 

#toy example:
set.seed(123)
m &lt;- 1000
n &lt;- 100
x &lt;- matrix(runif(m*n),m,n)
sol &lt;- rnorm(n)
y &lt;- rbinom(m, 1, prob = plogis(x%*%sol))
s2_granConj &lt;- LR_GradientConjugate_gpumatrix(X = x,y = y)

#the following compares LR_GradientConjugate_gpumatrix
# with glm and GPUglm:

require(MASS)
require(stats,quietly = TRUE)
#logistic:
data(menarche)
glm.out &lt;- glm(cbind(Menarche, Total-Menarche) ~ Age, family=binomial(), data=menarche)
summary(glm.out)

glm.out_gpu &lt;- GPUglm(cbind(Menarche, Total-Menarche) ~ Age, family=binomial(), data=menarche)
summary(glm.out_gpu)

#can be also called using glm.fit.gpu:
new_menarche &lt;- data.frame(Age=rep(menarche$Age,menarche$Total))
observations &lt;- c()
for(i in 1:nrow(menarche)){
  observations &lt;- c(observations,rep(c(0,1),c(menarche$Total[i]-menarche$Menarche[i],
                                              menarche$Menarche[i])))
}
new_menarche$observations &lt;- observations
x &lt;- model.matrix(~Age,data=new_menarche)
head(new_menarche)
glm.fit_gpu &lt;- glm.fit.GPU(x=x,y=new_menarche$observations, family=binomial())
summary(glm.fit_gpu)

#GPUmatrix package also include the function 'LR_GradientConjugate_gpumatrix'
lr_gran_sol &lt;- LR_GradientConjugate_gpumatrix(X = x,y = observations)

#check results
glm.out$coefficients
glm.out_gpu$coefficients
glm.fit_gpu$coefficients
lr_gran_sol


## End(Not run)

</code></pre>


</div>