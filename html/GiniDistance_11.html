<div class="container">

<table style="width: 100%;"><tr>
<td>KdCov</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kernel Distance Covariance Statistics</h2>

<h3>Description</h3>

<p>Computes Kernel distance covariance statistics,
in which Xs are quantitative, Y are categorical, sigma is kernel standard deviation and returns the measures of dependence.
</p>


<h3>Usage</h3>

<pre><code class="language-R">  KdCov(x, y, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p> data </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p> label of data or univariate response variable</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma</code></td>
<td>
<p> kernel standard deviation</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>KdCov</code> compute distance correlation statistics.
The sample size (number of rows) of the data must agree with the length of the label vector, and samples must not contain missing values. Arguments
<code>x</code>, <code>y</code> are treated as data and labels. 
</p>
<p>Distance covariance was introduced in (<cite>Szekely07</cite>) as a dependence measure between random variables <code class="reqn">X \in {R}^p</code> and <code class="reqn">Y \in {R}^q</code>. If <code class="reqn">X</code> and <code class="reqn">Y</code> are embedded into RKHS's induced by <code class="reqn">\kappa_X</code> and <code class="reqn">\kappa_Y</code>, respectively, the generalized distance covariance of <code class="reqn">X</code> and <code class="reqn">Y</code> is (<cite>Sejdinovic13</cite>):
</p>
<p style="text-align: center;"><code class="reqn">\begin{array}{c}
    \mathrm{dCov}_{\kappa_X,\kappa_Y}(X,Y) = {E}d_{\kappa_X}(X,X^{\prime})d_{\kappa_Y}(Y,Y^{\prime}) + {E}d_{\kappa_X}(X,X^{\prime}){E}d_{\kappa_Y}(Y,Y^{\prime}) \\
    - 2{E}\left[{E}_{X^{\prime}}d_{\kappa_X}(X,X^{\prime}) {E}_{Y^{\prime}}d_{\kappa_Y}(Y,Y^{\prime})\right].
    \end{array}
  </code>
</p>

<p>In the case of <code class="reqn">Y</code> being categorical, one may embed it using a set difference kernel <code class="reqn">\kappa_Y</code>,
</p>
<p style="text-align: center;"><code class="reqn">
    \kappa_Y(y,y^{\prime}) = \left\{ \begin{array}{cc}
    \frac{1}{2} &amp;  if \;y = y^{\prime},\\ 0 &amp; otherwise.
    \end{array} \right.
  </code>
</p>

<p>This is equivalent to embedding <code class="reqn">Y</code> as a simplex with edges of unit length (<cite>Lyons13</cite>), i.e., <code class="reqn">L_k</code> is represented by a <code class="reqn">K</code> dimensional vector of all zeros except its <code class="reqn">k</code>-th dimension, which has the value <code class="reqn">\frac{\sqrt{2}}{2}</code>. 
The distance induced by <code class="reqn">\kappa_Y</code> is called the set distance, i.e., <code class="reqn">d_{\kappa_Y}(y,y^{\prime})=0</code> if <code class="reqn">y=y^{\prime}</code> and <code class="reqn">1</code> otherwise. Using the set distance, we have the following results on the generalized distance covariance between a numerical 
and a categorical random variable.
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{dCov}_{\kappa_X,\kappa_Y}(X,Y) := \mathrm{dCov}_{\kappa_X}(X,Y) \nonumber = \sum_{k=1}^{K} p_k^2 \left[2 {E}d_{\kappa_X}(X_k,X) - {E}d_{\kappa_X}(X_k,{X_k}^{\prime}) - {E}d_{\kappa_X}(X,X^{\prime}) \right].</code>
</p>



<h3>Value</h3>

<p><code>KdCov</code> returns the sample kernel distance correlation
</p>


<h3>References</h3>

<p>Sejdinovic, D., Sriperumbudur, B., Gretton, A.  and Fukumizu, K. (2013). Equivalence of Distance-based and RKHS-based Statistics in Hypothesis Testing, The Annals of Statistics, 41 (5),  2263-2291. 
</p>
<p>Zhang, S., Dang, X., Nguyen, D. and Chen, Y. (2019). Estimating feature - label dependence using Gini distance statistics. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (submitted)</em>.
</p>


<h3>See Also</h3>

<p><code>KgCov</code>  <code>KgCor</code>  <code>dCov</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">  x&lt;-iris[,1:4]
  y&lt;-unclass(iris[,5])
  KdCov(x, y, sigma=1)
</code></pre>


</div>