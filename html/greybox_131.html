<div class="container">

<table style="width: 100%;"><tr>
<td>ME</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Error measures</h2>

<h3>Description</h3>

<p>Functions allow to calculate different types of errors for point and
interval predictions:
</p>

<ol>
<li>
<p> ME - Mean Error,
</p>
</li>
<li>
<p> MAE - Mean Absolute Error,
</p>
</li>
<li>
<p> MSE - Mean Squared Error,
</p>
</li>
<li>
<p> MRE - Mean Root Error (Kourentzes, 2014),
</p>
</li>
<li>
<p> MIS - Mean Interval Score (Gneiting &amp; Raftery, 2007),
</p>
</li>
<li>
<p> MPE - Mean Percentage Error,
</p>
</li>
<li>
<p> MAPE - Mean Absolute Percentage Error (See Svetunkov, 2017 for
the critique),
</p>
</li>
<li>
<p> MASE - Mean Absolute Scaled Error (Hyndman &amp; Koehler, 2006),
</p>
</li>
<li>
<p> RMSSE - Root Mean Squared Scaled Error (used in M5 Competition),
</p>
</li>
<li>
<p> rMAE - Relative Mean Absolute Error (Davydenko &amp; Fildes, 2013),
</p>
</li>
<li>
<p> rRMSE - Relative Root Mean Squared Error,
</p>
</li>
<li>
<p> rAME - Relative Absolute Mean Error,
</p>
</li>
<li>
<p> rMIS - Relative Mean Interval Score,
</p>
</li>
<li>
<p> sMSE - Scaled Mean Squared Error (Petropoulos &amp; Kourentzes, 2015),
</p>
</li>
<li>
<p> sPIS- Scaled Periods-In-Stock (Wallstrom &amp; Segerstedt, 2010),
</p>
</li>
<li>
<p> sCE - Scaled Cumulative Error,
</p>
</li>
<li>
<p> sMIS - Scaled Mean Interval Score,
</p>
</li>
<li>
<p> GMRAE - Geometric Mean Relative Absolute Error.
</p>
</li>
</ol>
<h3>Usage</h3>

<pre><code class="language-R">ME(holdout, forecast, na.rm = TRUE)

MAE(holdout, forecast, na.rm = TRUE)

MSE(holdout, forecast, na.rm = TRUE)

MRE(holdout, forecast, na.rm = TRUE)

MIS(holdout, lower, upper, level = 0.95, na.rm = TRUE)

MPE(holdout, forecast, na.rm = TRUE)

MAPE(holdout, forecast, na.rm = TRUE)

MASE(holdout, forecast, scale, na.rm = TRUE)

RMSSE(holdout, forecast, scale, na.rm = TRUE)

rMAE(holdout, forecast, benchmark, na.rm = TRUE)

rRMSE(holdout, forecast, benchmark, na.rm = TRUE)

rAME(holdout, forecast, benchmark, na.rm = TRUE)

rMIS(holdout, lower, upper, benchmarkLower, benchmarkUpper, level = 0.95,
  na.rm = TRUE)

sMSE(holdout, forecast, scale, na.rm = TRUE)

sPIS(holdout, forecast, scale, na.rm = TRUE)

sCE(holdout, forecast, scale, na.rm = TRUE)

sMIS(holdout, lower, upper, scale, level = 0.95, na.rm = TRUE)

GMRAE(holdout, forecast, benchmark, na.rm = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>holdout</code></td>
<td>
<p>The vector or matrix of holdout values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>forecast</code></td>
<td>
<p>The vector or matrix of forecasts values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.rm</code></td>
<td>
<p>Logical, defining whether to remove the NAs from the provided data or not.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower</code></td>
<td>
<p>The lower bound of the prediction interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>upper</code></td>
<td>
<p>The upper bound of the prediction interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>level</code></td>
<td>
<p>The confidence level of the constructed interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>The value that should be used in the denominator of MASE. Can
be anything but advised values are: mean absolute deviation of in-sample one
step ahead Naive error or mean absolute value of the in-sample actuals.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>benchmark</code></td>
<td>
<p>The vector or matrix of the forecasts of the benchmark
model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>benchmarkLower</code></td>
<td>
<p>The lower bound of the prediction interval of the
benchmark model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>benchmarkUpper</code></td>
<td>
<p>The upper bound of the prediction interval of the
benchmark model.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>In case of <code>sMSE</code>, <code>scale</code> needs to be a squared value. Typical
one – squared mean value of in-sample actuals.
</p>
<p>If all the measures are needed, then measures function
can help.
</p>
<p>There are several other measures, see details of pinball
and hm.
</p>


<h3>Value</h3>

<p>All the functions return the scalar value.
</p>


<h3>Author(s)</h3>

<p>Ivan Svetunkov, <a href="mailto:ivan@svetunkov.ru">ivan@svetunkov.ru</a>
</p>


<h3>References</h3>


<ul>
<li>
<p> Kourentzes N. (2014). The Bias Coefficient: a new metric for forecast bias
<a href="https://kourentzes.com/forecasting/2014/12/17/the-bias-coefficient-a-new-metric-for-forecast-bias/">https://kourentzes.com/forecasting/2014/12/17/the-bias-coefficient-a-new-metric-for-forecast-bias/</a>
</p>
</li>
<li>
<p> Svetunkov, I. (2017). Naughty APEs and the quest for the holy grail.
<a href="https://openforecast.org/2017/07/29/naughty-apes-and-the-quest-for-the-holy-grail/">https://openforecast.org/2017/07/29/naughty-apes-and-the-quest-for-the-holy-grail/</a>
</p>
</li>
<li>
<p> Fildes R. (1992). The evaluation of
extrapolative forecasting methods. International Journal of Forecasting, 8,
pp.81-98.
</p>
</li>
<li>
<p> Hyndman R.J., Koehler A.B. (2006). Another look at measures of
forecast accuracy. International Journal of Forecasting, 22, pp.679-688.
</p>
</li>
<li>
<p> Petropoulos F., Kourentzes N. (2015). Forecast combinations for
intermittent demand. Journal of the Operational Research Society, 66,
pp.914-924.
</p>
</li>
<li>
<p> Wallstrom P., Segerstedt A. (2010). Evaluation of forecasting error
measurements and techniques for intermittent demand. International Journal
of Production Economics, 128, pp.625-636.
</p>
</li>
<li>
<p> Davydenko, A., Fildes, R. (2013). Measuring Forecasting Accuracy:
The Case Of Judgmental Adjustments To Sku-Level Demand Forecasts.
International Journal of Forecasting, 29(3), 510-522.
<a href="https://doi.org/10.1016/j.ijforecast.2012.09.002">doi:10.1016/j.ijforecast.2012.09.002</a>
</p>
</li>
<li>
<p> Gneiting, T., &amp; Raftery, A. E. (2007). Strictly proper scoring rules,
prediction, and estimation. Journal of the American Statistical Association,
102(477), 359–378. <a href="https://doi.org/10.1198/016214506000001437">doi:10.1198/016214506000001437</a>
</p>
</li>
</ul>
<h3>See Also</h3>

<p>pinball, hm, measures
</p>


<h3>Examples</h3>

<pre><code class="language-R">

y &lt;- rnorm(100,10,2)
testForecast &lt;- rep(mean(y[1:90]),10)

MAE(y[91:100],testForecast)
MSE(y[91:100],testForecast)

MPE(y[91:100],testForecast)
MAPE(y[91:100],testForecast)

# Measures from Petropoulos &amp; Kourentzes (2015)
MASE(y[91:100],testForecast,mean(abs(y[1:90])))
sMSE(y[91:100],testForecast,mean(abs(y[1:90]))^2)
sPIS(y[91:100],testForecast,mean(abs(y[1:90])))
sCE(y[91:100],testForecast,mean(abs(y[1:90])))

# Original MASE from Hyndman &amp; Koehler (2006)
MASE(y[91:100],testForecast,mean(abs(diff(y[1:90]))))

testForecast2 &lt;- rep(y[91],10)
# Relative measures, from and inspired by Davydenko &amp; Fildes (2013)
rMAE(y[91:100],testForecast2,testForecast)
rRMSE(y[91:100],testForecast2,testForecast)
rAME(y[91:100],testForecast2,testForecast)
GMRAE(y[91:100],testForecast2,testForecast)

#### Measures for the prediction intervals
# An example with mtcars data
ourModel &lt;- alm(mpg~., mtcars[1:30,], distribution="dnorm")
ourBenchmark &lt;- alm(mpg~1, mtcars[1:30,], distribution="dnorm")

# Produce predictions with the interval
ourForecast &lt;- predict(ourModel, mtcars[-c(1:30),], interval="p")
ourBenchmarkForecast &lt;- predict(ourBenchmark, mtcars[-c(1:30),], interval="p")

MIS(mtcars$mpg[-c(1:30)],ourForecast$lower,ourForecast$upper,0.95)
sMIS(mtcars$mpg[-c(1:30)],ourForecast$lower,ourForecast$upper,mean(mtcars$mpg[1:30]),0.95)
rMIS(mtcars$mpg[-c(1:30)],ourForecast$lower,ourForecast$upper,
       ourBenchmarkForecast$lower,ourBenchmarkForecast$upper,0.95)

### Also, see pinball function for other measures for the intervals

</code></pre>


</div>