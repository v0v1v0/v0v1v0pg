<div class="container">

<table style="width: 100%;"><tr>
<td>predict.causal_forest</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Predict with a causal forest</h2>

<h3>Description</h3>

<p>Gets estimates of tau(x) using a trained causal forest.
</p>


<h3>Usage</h3>

<pre><code class="language-R">## S3 method for class 'causal_forest'
predict(
  object,
  newdata = NULL,
  linear.correction.variables = NULL,
  ll.lambda = NULL,
  ll.weight.penalty = FALSE,
  num.threads = NULL,
  estimate.variance = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>The trained forest.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>linear.correction.variables</code></td>
<td>
<p>Optional subset of indexes for variables to be used in local
linear prediction. If NULL, standard GRF prediction is used. Otherwise,
we run a locally weighted linear regression on the included variables.
Please note that this is a beta feature still in development, and may slow down
prediction considerably. Defaults to NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ll.lambda</code></td>
<td>
<p>Ridge penalty for local linear predictions. Defaults to NULL and will be cross-validated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ll.weight.penalty</code></td>
<td>
<p>Option to standardize ridge penalty by covariance (TRUE),
or penalize all covariates equally (FALSE). Penalizes equally by default.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num.threads</code></td>
<td>
<p>Number of threads used in training. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estimate.variance</code></td>
<td>
<p>Whether variance estimates for <code class="reqn">\hat\tau(x)</code> are desired
(for confidence intervals).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Vector of predictions, along with estimates of the error and
(optionally) its variance estimates. Column 'predictions' contains estimates
of the conditional average treatent effect (CATE). The square-root of
column 'variance.estimates' is the standard error of CATE.
For out-of-bag estimates, we also output the following error measures.
First, column 'debiased.error' contains estimates of the 'R-loss' criterion,
(See Nie and Wager, 2021 for a justification). Second, column 'excess.error'
contains jackknife estimates of the Monte-carlo error (Wager, Hastie, Efron 2014),
a measure of how unstable estimates are if we grow forests of the same size
on the same data set. The sum of 'debiased.error' and 'excess.error' is the raw error
attained by the current forest, and 'debiased.error' alone is an estimate of the error
attained by a forest with an infinite number of trees. We recommend that users grow
enough forests to make the 'excess.error' negligible.
</p>


<h3>References</h3>

<p>Friedberg, Rina, Julie Tibshirani, Susan Athey, and Stefan Wager. "Local Linear Forests".
Journal of Computational and Graphical Statistics, 30(2), 2020.
</p>
<p>Wager, Stefan, Trevor Hastie, and Bradley Efron.
"Confidence intervals for random forests: The jackknife and the infinitesimal jackknife."
The Journal of Machine Learning Research 15(1), 2014.
</p>
<p>Nie, Xinkun, and Stefan Wager. "Quasi-Oracle Estimation of Heterogeneous Treatment Effects".
Biometrika, 108(2), 2021.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Train a causal forest.
n &lt;- 100
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
c.forest &lt;- causal_forest(X, Y, W)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
c.pred &lt;- predict(c.forest, X.test)

# Predict on out-of-bag training samples.
c.pred &lt;- predict(c.forest)

# Predict with confidence intervals; growing more trees is now recommended.
c.forest &lt;- causal_forest(X, Y, W, num.trees = 500)
c.pred &lt;- predict(c.forest, X.test, estimate.variance = TRUE)


</code></pre>


</div>