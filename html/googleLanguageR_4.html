<div class="container">

<table style="width: 100%;"><tr>
<td>gl_speech</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Call Google Speech API</h2>

<h3>Description</h3>

<p>Turn audio into text
</p>


<h3>Usage</h3>

<pre><code class="language-R">gl_speech(
  audio_source,
  encoding = c("LINEAR16", "FLAC", "MULAW", "AMR", "AMR_WB", "OGG_OPUS",
    "SPEEX_WITH_HEADER_BYTE"),
  sampleRateHertz = NULL,
  languageCode = "en-US",
  maxAlternatives = 1L,
  profanityFilter = FALSE,
  speechContexts = NULL,
  asynch = FALSE,
  customConfig = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>audio_source</code></td>
<td>
<p>File location of audio data, or Google Cloud Storage URI</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>encoding</code></td>
<td>
<p>Encoding of audio data sent</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sampleRateHertz</code></td>
<td>
<p>Sample rate in Hertz of audio data. Valid values <code>8000-48000</code>. Optimal and default if left <code>NULL</code> is <code>16000</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>languageCode</code></td>
<td>
<p>Language of the supplied audio as a <code>BCP-47</code> language tag</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxAlternatives</code></td>
<td>
<p>Maximum number of recognition hypotheses to be returned. <code>0-30</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>profanityFilter</code></td>
<td>
<p>If <code>TRUE</code> will attempt to filter out profanities</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>speechContexts</code></td>
<td>
<p>An optional character vector of context to assist the speech recognition</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>asynch</code></td>
<td>
<p>If your <code>audio_source</code> is greater than 60 seconds, set this to TRUE to return an asynchronous call</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>customConfig</code></td>
<td>
<p>[optional] A <code>RecognitionConfig</code> object that will be converted from a list to JSON via <code>toJSON</code> - see <a href="https://cloud.google.com/speech-to-text/docs/reference/rest/v1p1beta1/RecognitionConfig">RecognitionConfig documentation</a>. The <code>languageCode</code> will be taken from this functions arguments if not present since it is required.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Google Cloud Speech API enables developers to convert audio to text by applying powerful
neural network models in an easy to use API.
The API recognizes over 80 languages and variants, to support your global user base.
You can transcribe the text of users dictating to an applicationâ€™s microphone,
enable command-and-control through voice, or transcribe audio files, among many other use cases.
Recognize audio uploaded in the request, and integrate with your audio storage on Google Cloud Storage,
by using the same technology Google uses to power its own products.
</p>


<h3>Value</h3>

<p>A list of two tibbles:  <code>$transcript</code>, a tibble of the <code>transcript</code> with a <code>confidence</code>; <code>$timings</code>, a tibble that contains <code>startTime</code>, <code>endTime</code> per <code>word</code>.  If maxAlternatives is greater than 1, then the transcript will return near-duplicate rows with other interpretations of the text.
If <code>asynch</code> is TRUE, then an operation you will need to pass to gl_speech_op to get the finished result.
</p>


<h3>AudioEncoding</h3>

<p>Audio encoding of the data sent in the audio message. All encodings support only 1 channel (mono) audio.
Only FLAC and WAV include a header that describes the bytes of audio that follow the header.
The other encodings are raw audio bytes with no header.
For best results, the audio source should be captured and transmitted using a
lossless encoding (FLAC or LINEAR16).
Recognition accuracy may be reduced if lossy codecs, which include the other codecs listed in this section,
are used to capture or transmit the audio, particularly if background noise is present.
</p>
<p>Read more on audio encodings here <a href="https://cloud.google.com/speech/docs/encoding">https://cloud.google.com/speech/docs/encoding</a>
</p>


<h3>WordInfo</h3>

<p><code>startTime</code> - Time offset relative to the beginning of the audio, and corresponding to the start of the spoken word.
</p>
<p><code>endTime</code> - Time offset relative to the beginning of the audio, and corresponding to the end of the spoken word.
</p>
<p><code>word</code> - The word corresponding to this set of information.
</p>


<h3>See Also</h3>

<p><a href="https://cloud.google.com/speech/reference/rest/v1/speech/recognize">https://cloud.google.com/speech/reference/rest/v1/speech/recognize</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 

test_audio &lt;- system.file("woman1_wb.wav", package = "googleLanguageR")
result &lt;- gl_speech(test_audio)

result$transcript
result$timings

result2 &lt;- gl_speech(test_audio, maxAlternatives = 2L)
result2$transcript

result_brit &lt;- gl_speech(test_audio, languageCode = "en-GB")


## make an asynchronous API request (mandatory for sound files over 60 seconds)
asynch &lt;- gl_speech(test_audio, asynch = TRUE)

## Send to gl_speech_op() for status or finished result
gl_speech_op(asynch)

## Upload to GCS bucket for long files &gt; 60 seconds
test_gcs &lt;- "gs://mark-edmondson-public-files/googleLanguageR/a-dream-mono.wav"
gcs &lt;- gl_speech(test_gcs, sampleRateHertz = 44100L, asynch = TRUE)
gl_speech_op(gcs)

## Use a custom configuration
my_config &lt;- list(encoding = "LINEAR16",
                  diarizationConfig = list(
                    enableSpeakerDiarization = TRUE,
                    minSpeakerCount = 2,
                    maxSpeakCount = 3
                    ))

# languageCode is required, so will be added if not in your custom config
gl_speech(my_audio, languageCode = "en-US", customConfig = my_config)


## End(Not run)



</code></pre>


</div>