<div class="container">

<table style="width: 100%;"><tr>
<td>trafos_dimreduction</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Dimension-Reduction Transformations for Training or Sampling</h2>

<h3>Description</h3>

<p>Dimension-reduction transformations applied to an input data matrix.
Currently on the principal component transformation and its inverse.
</p>


<h3>Usage</h3>

<pre><code class="language-R">PCA_trafo(x, mu, Gamma, inverse = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p><code class="reqn">(n, d)</code>-matrix of data (typically before training or
after sampling). If <code>inverse = FALSE</code>, then, conceptually,
an <code class="reqn">(n, d)</code>-matrix with <code class="reqn">1\le k \le d</code>,
where <code class="reqn">d</code> is the dimension of the original data whose dimension
was reduced to <code class="reqn">k</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>if <code>inverse = TRUE</code>, a <code class="reqn">d</code>-vector
of centers, where <code class="reqn">d</code> is the dimension to transform <code>x</code>
to.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Gamma</code></td>
<td>
<p>if <code>inverse = TRUE</code>, a <code class="reqn">(d, k)</code>-matrix with
<code class="reqn">k</code> at least as large as <code>ncol(x)</code> containing the
<code class="reqn">k</code> orthonormal eigenvectors of a covariance matrix sorted
in decreasing order of their eigenvalues; in other words,
the columns of <code>Gamma</code> contain principal axes or loadings.
If a matrix with <code class="reqn">k</code> greater than <code>ncol(x)</code> is provided,
only the first <code class="reqn">k</code>-many are considered.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>inverse</code></td>
<td>
<p><code>logical</code> indicating whether the inverse
transformation of the principal component transformation is applied.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments passed to the underlying
<code>prcomp()</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Conceptually, the principal component transformation transforms a
vector <code class="reqn">\bm{X}</code> to a vector <code class="reqn">\bm{Y}</code> where
<code class="reqn">\bm{Y} = \Gamma^T(\bm{X}-\bm{\mu})</code>,
where <code class="reqn">\bm{\mu}</code> is the mean vector of <code class="reqn">\bm{X}</code>
and <code class="reqn">\Gamma</code> is the <code class="reqn">(d, d)</code>-matrix whose
columns contains the orthonormal eigenvectors of <code>cov(X)</code>.
</p>
<p>The corresponding (conceptual) inverse transformation is
<code class="reqn">\bm{X} = \bm{\mu} + \Gamma \bm{Y}</code>.
</p>
<p>See McNeil et al. (2015, Section 6.4.5).
</p>


<h3>Value</h3>

<p>If <code>inverse = TRUE</code>, the transformed data whose rows contain
<code class="reqn">\bm{X} = \bm{\mu} + \Gamma \bm{Y}</code>, where
<code class="reqn">Y</code> is one row of <code>x</code>. See the details below for the
notation.
</p>
<p>If <code>inverse = FALSE</code>, a <code>list</code> containing:
</p>

<dl>
<dt>
<code>PCs</code>:</dt>
<dd>
<p><code class="reqn">(n, d)</code>-matrix of principal components.</p>
</dd>
<dt>
<code>cumvar</code>:</dt>
<dd>
<p>cumulative variances; the <code class="reqn">j</code>th entry
provides the fraction of the explained variance of the first
<code class="reqn">j</code> principal components.</p>
</dd>
<dt>
<code>sd</code>:</dt>
<dd>
<p>sample standard deviations of the transformed data.</p>
</dd>
<dt>
<code>lambda</code>:</dt>
<dd>
<p>eigenvalues of <code>cov(x)</code>.</p>
</dd>
<dt>
<code>mu</code>:</dt>
<dd>
<p><code class="reqn">d</code>-vector of centers of <code>x</code> (see also
above) typically provided to <code>PCA_trafo(, inverse = TRUE)</code>.</p>
</dd>
<dt>
<code>Gamma</code>:</dt>
<dd>
<p><code class="reqn">(d, d)</code>-matrix of principal axes (see also
above) typically provided to <code>PCA_trafo(, inverse = TRUE)</code>.</p>
</dd>
</dl>
<h3>Author(s)</h3>

<p>Marius Hofert</p>


<h3>References</h3>

<p>McNeil, A. J., Frey, R., and Embrechts, P. (2015).
<em>Quantitative Risk Management: Concepts, Techniques, Tools</em>.
Princeton University Press.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(gnn) # for being standalone

## Generate data
library(copula)
set.seed(271)
X &lt;- qt(rCopula(1000, gumbelCopula(2, dim = 10)), df = 3.5)
pairs(X, gap = 0, pch = ".")

## Principal component transformation
PCA &lt;- PCA_trafo(X)
Y &lt;- PCA$PCs
PCA$cumvar[3] # fraction of variance explained by the first 3 principal components
which.max(PCA$cumvar &gt; 0.9) # number of principal components it takes to explain 90%

## Biplot (plot of the first two principal components = data transformed with
## the first two principal axes)
plot(Y[,1:2])

## Transform back and compare
X. &lt;- PCA_trafo(Y, mu = PCA$mu, Gamma = PCA$Gamma, inverse = TRUE)
stopifnot(all.equal(X., X))

## Note: One typically transforms back with only some of the principal axes
X. &lt;- PCA_trafo(Y[,1:3], mu = PCA$mu, # mu determines the dimension to transform to
                Gamma = PCA$Gamma, # must be of dim. (length(mu), k) for k &gt;= ncol(x)
                inverse = TRUE)
stopifnot(dim(X.) == c(1000, 10))
## Note: We (typically) transform back to the original dimension.
pairs(X., gap = 0, pch = ".") # pairs of back-transformed first three PCs
</code></pre>


</div>