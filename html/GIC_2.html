<div class="container">

<table style="width: 100%;"><tr>
<td>iteration</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>A General Iterative Clustering Algorithm</h2>

<h3>Description</h3>

<p>An algorithm that improves the proximity matrix (PM) from a random forest (RF) and the resulting clusters from an arbitrary cluster algorithm as measured by the silhouette score. The initial PM, that uses unlabeled data, is produced by one of many ways to provide psuedo labels for a RF. After running a cluster program on the resulting initial PM, cluster labels are obtained. These are used as labels with the same feature data to grow a new RF yielding an updated proximity matrix. This is entered into the clustering program and the process is repeated until convergence.
</p>


<h3>Usage</h3>

<pre><code class="language-R">iteration(data,initiallabel,ntree=500)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p> an input dataframe without label</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initiallabel</code></td>
<td>
<p> a vector of label to begin with</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntree</code></td>
<td>
<p> the number of trees (default 500).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This code requires initial labels as input, which can be obtained by any method of the users choice.
As an alternative, Breimans' unsupervised method or Siegel and her colleagues' purposeful clustering method to obtain initial labels, use the function <code>GIC</code>
</p>


<h3>Value</h3>

<p>An object of class <code>iteration</code>, which is a list with the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>PAM</code></td>
<td>
<p>output final PAM information</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>randomforest</code></td>
<td>
<p>output final randomforest information</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clustering</code></td>
<td>
<p>A vector of integers indicating the cluster to which each point is allocated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>silhouette_score</code></td>
<td>
<p>A value of mean silhouette score for clusters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot</code></td>
<td>
<p>A scatter plot which X-axis, y-axis, and color are first important feature, second important feature, and final clusters, respectively. </p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Breiman, L. (2001), Random Forests, <em>Machine Learning</em> 45(1),
5-32.
</p>
<p>Siegel, C.E., Laska, E.M., Lin, Z., Xu, M., Abu-Amara, D., Jeffers, M.K., Qian, M., Milton, N., Flory, J.D., Hammamieh, R. and Daigle, B.J., (2021). Utilization of machine learning for identifying symptom severity military-related PTSD subtypes and their biological correlates. <em>Translational psychiatry</em>, 11(1), pp.1-12.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data(iris)
##Using KMEANS to find inital label
cl=kmeans(iris[,1:4],3)
###Doing GIC to find final clustering
rs=iteration(iris[,1:4],cl$cluster,ntree=100)
print(rs$clustering)

</code></pre>


</div>