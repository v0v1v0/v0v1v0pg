<div class="container">

<table style="width: 100%;"><tr>
<td>inference</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Statistical Inference for Regularized Gaussian Graphical Models</h2>

<h3>Description</h3>

<p>Compute <em>p</em>-values for each relation based on the
de-sparsified glasso estimator (Jankova and Van De Geer 2015).
</p>


<h3>Usage</h3>

<pre><code class="language-R">inference(object, method = "fdr", alpha = 0.05, ...)

significance_test(object, method = "fdr", alpha = 0.05, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>An object of class <code>ggmncv</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>Character string. A correction method for multiple comparison (defaults to <code>fdr</code>).
Can be abbreviated. See p.adjust.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Numeric. Significance level (defaults to <code>0.05</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Currently ignored.</p>
</td>
</tr>
</table>
<h3>Value</h3>


<ul>
<li> <p><code>Theta</code> De-sparsified precision matrix
</p>
</li>
<li> <p><code>adj</code> Adjacency matrix based on the p-values.
</p>
</li>
<li> <p><code>pval_uncorrected</code> Uncorrected p-values
</p>
</li>
<li> <p><code>pval_corrected</code> Corrected p-values
</p>
</li>
<li> <p><code>method</code> The approach used for multiple comparisons
</p>
</li>
<li> <p><code>alpha</code> Significance level
</p>
</li>
</ul>
<h3>Note</h3>

<p>This assumes (reasonably) Gaussian data, and should not to be expected
to work for, say, polychoric correlations. Further, all work to date
has only looked at the graphical lasso estimator, and not de-sparsifying
nonconvex regularization. Accordingly, it is probably best to set
<code>penalty = "lasso"</code> in <code>ggmncv</code>.
</p>
<p>Further, whether the de-sparsified estimator provides nominal error rates
remains to be seen, at least across a range of conditions. For example,
the simulation results in Williams (2021)
demonstrated that the confidence intervals
can have (severely) compromised coverage properties (whereas non-regularized methods
had coverage at the nominal level).
</p>


<h3>References</h3>

<p>Jankova J, Van De Geer S (2015).
“Confidence intervals for high-dimensional inverse covariance estimation.”
<em>Electronic Journal of Statistics</em>, <b>9</b>(1), 1205–1229.<br><br> Williams DR (2021).
“The Confidence Interval that Wasn't: Bootstrapped "Confidence Intervals" in L1-Regularized Partial Correlation Networks.”
<em>PsyArXiv</em>.
doi: <a href="https://doi.org/10.31234/osf.io/kjh2f">10.31234/osf.io/kjh2f</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># data
Y &lt;- GGMncv::ptsd[,1:5]

# fit model
fit &lt;- ggmncv(cor(Y), n = nrow(Y),
              progress = FALSE,
              penalty = "lasso")


# statistical inference
inference(fit)

# alias
all.equal(inference(fit), significance_test(fit))

</code></pre>


</div>