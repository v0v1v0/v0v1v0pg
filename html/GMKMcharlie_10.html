<div class="container">

<table style="width: 100%;"><tr>
<td>KMsparse</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
K-means over sparse representation of data
</h2>

<h3>Description</h3>

<p>Multithreaded weighted Minkowski and spherical K-means via Lloyd's algorithm over sparse representation of data.
</p>


<h3>Usage</h3>

<pre><code class="language-R">KMsparse(
  X,
  d,
  centroid,
  Xw = rep(1, length(X)),
  minkP = 2,
  maxIter = 100L,
  maxCore = 7L,
  verbose = TRUE
  )
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>A list of size <code>N</code>, the number of observations. <code>X[[i]]</code> is a 2-column data frame. The 1st column is a sorted <strong>integer vector</strong> of the indexes of nonzero dimensions. Values in these dimensions are stored in the 2nd column as a <strong>numeric vector</strong>. Internally the algorithm sets a 32-bit <em>int</em> pointer to the beginning of the 1st column and a 64-bit <em>double</em> pointer to the beginning of the 2nd column, so it is critical that the input has the correct type.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>

<p>An integer. The dimensionality of <code>X</code>. <code>d</code> MUST be no less than the maximum of all index vectors in <code>X</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>centroid</code></td>
<td>

<p>A list of size <code>K</code>, the number of clusters. <code>centroid[[i]]</code> can be in dense or sparse representation. If dense, a numeric vector of size <code>d</code>. If sparse, a 2-column data frame in the same sense as <code>X[[i]]</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Xw</code></td>
<td>

<p>A numeric vector of size <code>N</code>. <code>Xw[i]</code> is the weight on observation <code>X[[i]]</code>. Users should normalize <code>Xw</code> such that the elements sum up to <code>N</code>. Default uniform weights for all observations.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minkP</code></td>
<td>

<p>A numeric value or a character string. If numeric, <code>minkP</code> is the power <code>p</code> in the definition of Minkowski distance. If character string, <code>"max"</code> implies Chebyshev distance, <code>"cosine"</code> implies cosine dissimilarity. Default 2.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxIter</code></td>
<td>

<p>An integer. The maximal number of iterations. Default 100.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxCore</code></td>
<td>

<p>An integer. The maximal number of threads to invoke. No more than the total number of logical processors on machine. Default 7.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>

<p>A boolean value. <code>TRUE</code> prints progress.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>See details in <code>KM()</code> for implementation highlights. There are some other optimizations such as, except for the maximum norm, cost of computing the distance between a dense centroid vector and a sparse observation is linear to the size of the sparse observation, which should be largely less than the size of the dense vector. This is done by letting every centroid memorize its before-root Minkowski norm. The full distance can then be inferred from adding the residual norm to the partial distance.
</p>


<h3>Value</h3>

<p>A list of size <code>K</code>, the number of clusters. Each element is a list of 3 vectors:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>centroid </code></td>
<td>
<p>a numeric vector of size <code>d</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clusterMember </code></td>
<td>
<p>an integer vector of indexes of elements grouped to <code>centroid</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>member2centroidDistance </code></td>
<td>
<p>a numeric vector of the same size of <code>clusterMember</code>. The <code>i</code>th element is the Minkowski distance or cosine dissimilarity from <code>centroid</code> to the <code>clusterMember[i]</code>th observation in <code>X</code>.</p>
</td>
</tr>
</table>
<p>Empty <code>clusterMember</code> implies empty cluster.
</p>


<h3>Note</h3>

<p>Although rarely happens, divergence of K-means with non-Euclidean distance <code>minkP != 2</code> measure is still a theoretical possibility.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># ===========================================================================
# Play random numbers. See speed.
# ===========================================================================
N = 10000L # Number of points.
d = 500L # Dimensionality.
K = 100L # Number of clusters.


# Create a data matrix, about 95% of which are zeros.
dat = matrix(unlist(lapply(1L : N, function(x)
{
  tmp = numeric(d)
  # Nonzero entries.
  Nnz = as.integer(max(1, d * runif(1, 0, 0.05)))
  tmp[sample(d, Nnz)] = runif(Nnz) + rnorm(Nnz)
  tmp
})), nrow = d); gc()


# Convert to sparse representation.
# GMKMcharlie::d2s() acheives the same.
sparsedat = apply(dat, 2, function(x)
{
  nonz = which(x != 0)
  list(nonz, x[nonz])
}); gc()


centroidInd = sample(length(sparsedat), K)


# Test speed using dense representation.
centroid = dat[, centroidInd]
system.time({rst = GMKMcharlie::KM(
  X = dat, centroid = centroid, maxIter = 100,
  minkP = 2, maxCore = 2, verbose = TRUE)})


# Test speed using sparse representation.
sparseCentroid = sparsedat[centroidInd]
system.time({sparseRst = GMKMcharlie::KMsparse(
  X = sparsedat, d = d, centroid = sparseCentroid,
  maxIter = 100, minkP = 2, maxCore = 2, verbose = TRUE)})
</code></pre>


</div>