<div class="container">

<table style="width: 100%;"><tr>
<td>chat.edit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Chat Edit with New Text as the Input</h2>

<h3>Description</h3>

<p>This function establishes a connection to a generative AI model through a generative AI object.
It generates a chat response based on the new prompt and stores it in the chat history along
with the generative AI object.
</p>


<h3>Usage</h3>

<pre><code class="language-R">chat.edit(
  genai.object,
  prompt,
  message.to.edit,
  verbose = FALSE,
  config = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prompt</code></td>
<td>
<p>A character string representing the query for chat generation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>message.to.edit</code></td>
<td>
<p>An integer representing the index of the message to be edited.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Optional. Default to <code>FALSE</code>. A boolean value determining whether or not to print
out the details of the chat request.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>config</code></td>
<td>
<p>Optional. Default to <code>list()</code>. A list of configuration parameters for chat generation.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful chat
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code>available.models</code>.
</p>
<p>In addition, this function modifies the chat history along with the generative AI object directly,
meaning the chat history is mutable. You can print out the chat history using the
function <code>chat.history.print</code> or simply use <code>verbose = TRUE</code> in this function. To reset the chat history along with
the chat history along with the generative AI object, use the function <code>chat.history.reset</code>.
</p>
<p>For <strong>Google Generative AI</strong> models, available configurations are as follows. For more detail,
please refer
to <code>https://ai.google.dev/api/rest/v1/HarmCategory</code>,
<code>https://ai.google.dev/api/rest/v1/SafetySetting</code>, and
<code>https://ai.google.dev/api/rest/v1/GenerationConfig</code>.
</p>

<ul>
<li> <p><code>harm.category.dangerous.content</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for dangerous content,
with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.harassment</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for harasment content,
with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.hate.speech</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for hate speech and
content, with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.sexually.explicit</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for sexually explicit
content, with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>stop.sequences</code>
</p>
<p>Optional. A list of character sequences (up to 5) that will stop output generation. If specified,
the API will stop at the first appearance of a stop sequence. The stop sequence will not be
included as part of the response.
</p>
</li>
<li> <p><code>max.output.tokens</code>
</p>
<p>Optional. An integer, value varies by model, representing maximum number of tokens to include
in a candidate.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number, from 0.0 to 1.0 inclusive, controlling the randomness of the output.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number, value varies by model, representing maximum cumulative probability of tokens
to consider when sampling.
</p>
</li>
<li> <p><code>top.k</code>
</p>
<p>Optional. A number, value varies by model, representing maximum number of tokens to consider when sampling.
</p>
</li>
</ul>
<p>For <strong>Moonshot AI</strong> models, available configurations are as follows. For more detail, please refer to
<code>https://platform.moonshot.cn/api.html#chat-completion</code>.
</p>

<ul>
<li> <p><code>max.tokens</code>
</p>
<p>Optional. An integer. The maximum number of tokens that will be generated when the chat completes.
If the chat is not finished by the maximum number of tokens generated, the finish reason will be
"length", otherwise it will be "stop".
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number. What sampling temperature to use, between 0 and 1. Higher values (e.g. 0.7) will
make the output more random, while lower values (e.g. 0.2) will make it more focused and deterministic.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number. Another sampling temperature.
</p>
</li>
</ul>
<p>For <strong>OpenAI</strong> models, available configurations are as follows. For more detail, please refer to
<code>https://platform.openai.com/docs/api-reference/chat/create</code>.
</p>

<ul>
<li> <p><code>frequency.penalty</code>
</p>
<p>Optional. A number from -2.0 to 2.0 inclusive. Positive values penalize new tokens based on their
existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
</p>
</li>
<li> <p><code>logit.bias</code>
</p>
<p>Optional. A map. Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object
that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to
100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact
effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
</p>
</li>
<li> <p><code>logprobs</code>
</p>
<p>Optional. A boolean value. Whether to return log probabilities of the output tokens or not. If true, returns the log
probabilities of each output token returned in the content of message
</p>
</li>
<li> <p><code>top.logprobs</code>
</p>
<p>Optional. An integer between 0 and 5 specifying the number of most likely tokens to return at each token
position, each with an associated log probability. <code>logprobs</code> must be set to <code>TRUE</code> if this
parameter is used.
</p>
</li>
<li> <p><code>max.tokens</code>
</p>
<p>Optional. An integer. The maximum number of tokens that can be generated in the chat completion. The total length of
input tokens and generated tokens is limited by the model's context length.
</p>
</li>
<li> <p><code>presence.penalty</code>
</p>
<p>Optional. A Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
in the text so far, increasing the model's likelihood to talk about new topics.
</p>
</li>
<li> <p><code>response.format</code>
</p>
<p>Optional. An object specifying the format that the model must output. Compatible with GPT-4 Turbo and
all GPT-3.5 Turbo models newer than <code>gpt-3.5-turbo-1106</code>.
</p>
</li>
<li> <p><code>seed</code>
</p>
<p>Optional. An integer. If specified, our system will make a best effort to sample deterministically, such that repeated
requests with the same seed and parameters should return the same result.
</p>
</li>
<li> <p><code>stop</code>
</p>
<p>Optional. A character string or list contains up to 4 sequences where the API will stop generating further tokens.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
more random, while lower values like 0.2 will make it more focused and deterministic.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number. An alternative to sampling with temperature, called nucleus sampling, where the model considers
the results of the tokens with <code>top.p</code> probability mass. So 0.1 means only the tokens comprising the top
10
</p>
</li>
<li> <p><code>tools</code>
</p>
<p>Optional. A list of tools the model may call. Currently, only functions are supported as a tool. Use this
to provide a list of functions the model may generate JSON inputs for.
</p>
</li>
<li> <p><code>tool.choice</code>
</p>
<p>Optional. A character string or object. Controls which (if any) function is called by the model. <code>none</code> means
the model will not call a function and instead generates a message. <code>auto</code> means the model can pick
between generating a message or calling a function.
</p>
</li>
<li> <p><code>user</code>
</p>
<p>Optional. A character string. A unique identifier representing your end-user, which can help OpenAI to monitor
and detect abuse.
</p>
</li>
</ul>
<h3>Value</h3>

<p>If successful, the most recent chat response will be returned. If the API response indicates
an error, the function halts execution and provides an error message.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package "GenAI" Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/chat_edit.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, please refer to the "Live Demo in Colab" above for real
# examples. The following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
genai.model %&gt;%
  chat.edit(prompt = "What is XGBoost?",
            message.to.edit = 5,
            verbose = TRUE,
            config = parameters) %&gt;%
  cat()

# Method 2: use the reference operator "$"
cat(genai.model$chat.edit(prompt = "What is CatBoost?",
                          message.to.edit = 3))

# Method 3: use the function chat.edit() directly
cat(chat.edit(genai.object = genai.model,
              prompt = "What is LightGBM?",
              message.to.edit = 1))

## End(Not run)

</code></pre>


</div>