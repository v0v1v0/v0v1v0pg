<div class="container">

<table style="width: 100%;"><tr>
<td>multimin</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Function minimization</h2>

<h3>Description</h3>

<p><em>These functions have been removed from the package temporarily,
pending a permanent fix.</em>
</p>
<p>Function minimization using the Gnu Scientific Library, reference
manual section 35.  These functions are declared in header file
<code>gsl_multimin.h</code>
</p>
<p>Several algorithms for finding (local) minima of functions in one or
more variables are provided.  All of the algorithms operate locally,
in the sense that they maintain a best guess and require the function
to be continuous.  Apart from the Nelder-Mead algorithm, these
algorithms also use a derivative.
</p>


<h3>Usage</h3>

<pre><code class="language-R">multimin(..., prec=0.0001)
multimin.init(x, f, df=NA, fdf=NA, method=NA, step.size=NA, tol=NA)
multimin.iterate(state)
multimin.restart(state)
multimin.fminimizer.size(state)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>In function <code>multimin()</code>, the argument list passed to
<code>multimin.init()</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A starting point.  These algorithms are faster with better
initial guesses</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>f</code></td>
<td>
<p>The function to minimize.  This function must take a single
<code>numeric</code> vector as input, and output a <code>numeric</code> scalar</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>The derivative of <code>f</code>.  This is required for all algorithms
except Nelder-Mead</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fdf</code></td>
<td>
<p>A function that evaluates <code>f</code> and <code>df</code> simultaneously.
This is optional, and is only useful if simultaneous evaluation is faster</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>The algorithm to use, which is one of
“<code>conjugate-fr</code>”, “<code>conjugate-pr</code>”,
“<code>bfgs</code>”, “<code>steepest-descent</code>” and
“<code>nm</code>”</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step.size</code></td>
<td>
<p>This step size guides the algorithm to pick a good
distance between points in its search</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>This parameter is relevant for gradient-based methods.  It
controls how much the gradient should flatten out in each line
search.  More specifically, let <code class="reqn">u(t) = f(x + st)</code> be the
function restricted to the search ray.  Then a point <code class="reqn">t</code> is
tolerable if <code class="reqn">u'(t) &lt; tol u'(0)</code>.  Higher values give more lax
linesearches.  This parameter trades-off searching intensively in
the outer loop (finding search directions) versus the inner loop
(finding a good point in a particular direction)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prec</code></td>
<td>
<p>The stopping-rule precision parameter.  For the derivative-based
methods, a solution is good enough if the norm of the gradient is smaller
than <code>prec</code>.  For the non-derivative-based methods, a solution is good
enough if the norm of successive solutions is smaller than <code>prec</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>state</code></td>
<td>
<p>This stores all information relating to the progress of
the optimization problem</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>There are two ways to call <code>multimin</code>.  The simple way is to
merely call <code>multimin</code> directly.  A more complicated way is to
call <code>multimin.init</code> first, and then repeatedly call
<code>multimin.iterate</code> until the guess gets good enough.  In
addition, <code>multimin.restart</code> can be used with the second approach
to discard accumulated information (such as curvature information) if
that information turns out to be unhelpful.  This is roughly
equivalent to calling <code>multimin.init</code> by setting the starting
point to be the current best guess.
</p>
<p>All of the derivative-based methods consist of iterations that pick a
descent direction, and conduct a line search for a better point along
the ray in that direction from the current point.  The Fletcher-Reeves
and Polak-Ribiere conjugate gradient algorithms maintain a a vector
that summarizes the curvature at that point.  These are useful for
high-dimensional problems (eg: more than 100 dimensions) because they
don't use matrices which become expensive to keep track of.  The
Broyden-Fletcher-Goldfarb-Shanno is better for low-dimensional
problems, since it maintains an approximation of the Hessian of the
function as well, which gives better curvature information.  The
steepest-descent algorithm is a naive algorithm that does not use any
curvature information.  The Nelder-Mead algorithm which does not use
derivatives.
</p>


<h3>Value</h3>

<p>All of these functions return a state variable, which consists
of the following items:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>internal.state</code></td>
<td>
<p>Bureaucratic stuff for communicating with <abbr><span class="acronym">GSL</span></abbr></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>The current best guess of the optimal solution</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>f</code></td>
<td>
<p>The value of the function at the best guess</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>The derivative of the function at the best guess</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>is.fdf</code></td>
<td>
<p>TRUE if the algorithm is using a derivative</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>code</code></td>
<td>
<p>The <abbr><span class="acronym">GSL</span></abbr> return code from the last iteration</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>The source code for the functions documented here conditionalizes
on <code>WIN32</code>; under windows there is a slight memory leak.</p>


<h3>Author(s)</h3>

<p>Andrew Clausen <a href="mailto:clausen@econ.upenn.edu">clausen@econ.upenn.edu</a></p>


<h3>References</h3>

<p><a href="https://www.gnu.org/software/gsl/">https://www.gnu.org/software/gsl/</a></p>


<h3>See Also</h3>

<p><code>optim</code> and <code>nlm</code> are the standard optimization functions in
R.
</p>
<p><code>deriv</code> and <code>D</code> are the standard symbolic differentation
functions in R.  <code>Ryacas</code> provides more extensive differentiation
support using Yet Another Computer Algebra System.
</p>
<p><code>numericDeriv</code> is the standard numerical differentation function
in R.  <abbr><span class="acronym">GSL</span></abbr> can also do numerical differentiation, but no-one has
written an R interface yet.
</p>
<p><code>multimin</code> requires the objective function to have a single
(vector) argument. <code>unlist</code> and <code>relist</code> are useful for
converting between more convenient forms.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# COMMENTED OUT PENDING PERMANENT FIX
# The Rosenbrock function:

# x0 &lt;- c(-1.2, 1)
# f &lt;- function(x) (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2
# df &lt;- function(x) c(-2*(1 - x[1]) + 100 * 2 * (x[2] - x[1]^2) * (-2*x[1]),
#                     100 * 2 * (x[2] - x[1]^2))
# 
# # The simple way to call multimin.
# state &lt;- multimin(x0, f, df)
# print(state$x)
# 
# # The fine-control way to call multimin.
# state &lt;- multimin.init(x0, f, df, method="conjugate-fr")
# for (i in 1:200)
# 	state &lt;- multimin.iterate(state)
# print(state$x)

</code></pre>


</div>