<div class="container">

<table style="width: 100%;"><tr>
<td>gnet</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
(Adaptive) elastic net in GAMLSS
</h2>

<h3>Description</h3>

<p>This function allows estimating the different components of a GAMLSS model (location, shape, scale parameters) using the (adaptive) elastic net (with adaptive lasso as default special case) estimation method via <code>glmnet</code>. This method is appropriate for models with many variables.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gnet( X = NULL, x.vars = NULL, lambda = NULL, method = c("IC", "CV"), 
  type = c("agg", "sel"), ICpen = c("BIC", "HQC", "AIC"), CVp = 2, 
  k.se = 0, adaptive = 1, epsilon = 1/sqrt(dim(X)[1]), subsets = NULL, 
  sparse = FALSE, control = gnet.control(...), ...) 
gnet.control(family="gaussian", offset = NULL, alpha = 1, nlambda = 100, 
  lambda.min.ratio = 1e-3, standardize = TRUE, intercept = TRUE, thresh = 1e-07,
  dfmax = NULL, pmax = NULL, exclude = NULL, penalty.factor = NULL, lower.limits = -Inf,
  upper.limits = Inf, maxit = 100000, type.gaussian = NULL, type.logistic = "Newton")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p> The data frame containing the explanatory variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.vars</code></td>
<td>
<p> Indicates the name of the variables that must be included as explanatory variables from data the data object of GAMLSS. The explanatory variables must be included by <code>X</code> or by <code>x.vars</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p> The provided lambda grid. By default <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p> The method used to calculate the optimal lambda. If <code>method="IC"</code> information criteria are used, the penalization for the information criterion is selected in <code>ICpen</code>.If <code>method="CV"</code> cross validation resp. sampling is used, the penalization for the cross-validation is selected in <code>CVp</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p> The way to select the optimal lambda across the subsample fits. If <code>type="sel"</code> the optimal lambda is computed by selection. If <code>method="agg"</code> the optimal lambda is computed by aggregation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ICpen</code></td>
<td>
<p> The penalization for the information criteria. If <code>ICpen="AIC"</code> or  <code>ICpen=2</code> the optimal lambda is computed by Akaike Information Criterion. If <code>ICpen="BIC"</code> the optimal lambda is computed by Bayesian Information Criterion.If <code>ICpen="HQC"</code> the optimal lambda is computed by Hannan-Quinn Information Criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CVp</code></td>
<td>
<p> The penalization for the cross-validation, establishes the power of the error term. By default is equal to 2, i.e. squared error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k.se</code></td>
<td>
<p> This parameter establishes how many times the standard deviation is summed to the mean to select the optimal lambda. By default is equal to 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adaptive</code></td>
<td>
<p> This parameter specifies if adaptive lasso shall be used, the default is 1. If <code>NULL</code> then standard lasso is used, otherwise adaptive lasso with penalty weights <code>(abs(beta)+epsilon)^(-adaptive)</code> where <code>beta</code> is chosen from an initial standard lasso estimate and <code>epsilon</code> is specified by the next parameter. Note, estimating standard lasso requires about half of computation time, but adaptive lasso has smaller bias and satiesfies the oracle property.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p> This parameter specifies the adaptive lasso penalty weights. The default is <code>1/sqrt(dim(X)[1])</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subsets</code></td>
<td>
<p> The subsets for cross-validation, information criteria or bootstraping, by default 5 random folds are selected.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sparse</code></td>
<td>
<p> If sparse converts input matrix for <code>glmnet</code> into a sparse Matrix, may reduces computation time for sparse designs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>List of further input parameters for glmnet, e.g. alpha for elastic net parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>for extra arguments</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>Either a character string representing
one of the built-in families, or else a <code>glm()</code> family object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>
<p>A vector of length <code>nobs</code> that is included in the linear
predictor (a <code>nobs x nc</code> matrix for the <code>"multinomial"</code> family).
Useful for the <code>"poisson"</code> family (e.g. log of exposure time), or for
refining a model by starting at a current fit. Default is <code>NULL</code>. If
supplied, then values must also be supplied to the <code>predict</code> function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The elastic net mixing parameter, with <code class="reqn">0\le\alpha\le 1</code>.
The penalty is defined as </p>
<p style="text-align: center;"><code class="reqn">(1-\alpha)/2||\beta||_2^2+\alpha||\beta||_1.</code>
</p>
 <p><code>alpha=1</code> is the
lasso penalty, and <code>alpha=0</code> the ridge penalty. Default is lasso.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>Size of the tuning parameter grid, default is 100. It is irrelevant if lambda is explicitly specified.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min.ratio</code></td>
<td>
<p>Smallest value for <code>lambda</code>, as a fraction of
<code>lambda.max</code>, the (data derived) entry value (i.e. the smallest value
for which all coefficients are zero). The default is <code>0.001</code>.  A very small value of
<code>lambda.min.ratio</code> will lead to a saturated fit in the <code>nobs &lt;
nvars</code> case. This is undefined for <code>"binomial"</code> and
<code>"multinomial"</code> models, and <code>glmnet</code> will exit gracefully when the
percentage deviance explained is almost 1. It is irrelevant if lambda is explicitly specified.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>Logical flag for <code>X</code> or <code>x.vars</code> variable standardization, prior to
fitting the model sequence. The coefficients are always returned on the
original scale. Default is <code>standardize=TRUE</code> and it is highly recommended.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>Should intercept(s) be fitted (default=TRUE) or set to zero
(FALSE).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thresh</code></td>
<td>
<p>Convergence threshold for coordinate descent. Each inner
coordinate-descent loop continues until the maximum change in the objective
after any coefficient update is less than <code>thresh</code> times the null
deviance. Defaults value is <code>1E-7</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfmax</code></td>
<td>
<p>Limit the maximum number of variables in the model. Useful for
very large <code>nvars</code>, if a partial path is desired.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pmax</code></td>
<td>
<p>Limit the maximum number of variables ever to be nonzero.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exclude</code></td>
<td>
<p>Indices of variables to be excluded from the model. Default
is none. Equivalent to an infinite penalty factor (next item).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty.factor</code></td>
<td>
<p>Separate penalty factors can be applied to each
coefficient. This is a number that multiplies <code>lambda</code> to allow
differential shrinkage. Can be 0 for some variables, which implies no
shrinkage, and that variable is always included in the model. Default is 1
for all variables (and implicitly infinity for variables listed in
<code>exclude</code>). Note: the penalty factors are internally rescaled to sum to
nvars, and the lambda sequence will reflect this change.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower.limits</code></td>
<td>
<p>Vector of lower limits for each coefficient; default
<code>-Inf</code>. Each of these must be non-positive. Can be presented as a
single value (which will then be replicated), else a vector of length
<code>nvars</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>upper.limits</code></td>
<td>
<p>Vector of upper limits for each coefficient; default
<code>Inf</code>. See <code>lower.limits</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>Maximum number of passes over the data for all lambda values;
default is 10^5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type.gaussian</code></td>
<td>
<p>Two algorithm types are supported for (only)
<code>family="gaussian"</code>. The default when <code>nvar&lt;500</code> is
<code>type.gaussian="covariance"</code>, and saves all inner-products ever
computed. This can be much faster than <code>type.gaussian="naive"</code>, which
loops through <code>nobs</code> every time an inner-product is computed. The
latter can be far more efficient for <code>nvar &gt;&gt; nobs</code> situations, or when
<code>nvar &gt; 500</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type.logistic</code></td>
<td>
<p>If <code>"Newton"</code> then the exact hessian is used
(default), while <code>"modified.Newton"</code> uses an upper-bound on the
hessian, and can be faster.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The estimation of the lambda is carried out by BIC by default.
If the objective is to predict the model must be defined by <code>x.vars</code>. 
Different types of subsets must be constructed if bootstrapping and aggregation are applied, as in this case observations might be repeated. 

</p>


<h3>Value</h3>

<p>This function returns a smooth object of the GAMLSS model. It contains the estimated parameters and related characteristics for the <code>glmnet</code> component in the GAMLSS model we are estimating.
</p>


<h3>Author(s)</h3>

<p>Florian Ziel, Peru Muniain and Mikis Stasinopoulos
</p>


<h3>References</h3>

<p>Rigby, R. A. and  Stasinopoulos D. M. (2005). Generalized additive models for location, scale, and shape,(with discussion), <em>Appl. Statist.</em>, <b>54</b>, part 3, pp 507-554.
</p>
<p>Rigby, R. A., Stasinopoulos, D. M., Heller, G. Z., and De Bastiani, F. (2019) Distributions for modeling location, scale, and shape: Using GAMLSS in R, Chapman and Hall/CRC. An older version can be found in https://www.gamlss.com/. 
</p>
<p>Stasinopoulos D. M. Rigby R.A. (2007) Generalized additive models for location scale and shape (GAMLSS) in R. <em>Journal of Statistical Software</em>, <b>Vol. 23</b>, Issue 7, Dec 2007, https://www.jstatsoft.org/v23/i07/. 
</p>
<p>Stasinopoulos D. M., Rigby R.A., Heller G., Voudouris V., and De Bastiani F., (2017) Flexible Regression and Smoothing: Using GAMLSS in R, Chapman and Hall/CRC. 
</p>
<p>Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011) Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent, <em>Journal of Statistical Software</em>, <b>Vol. 39(5)</b>, 1-13, https://www.jstatsoft.org/v39/i05/.
</p>
<p>Tibshirani, Robert, Bien, J., Friedman, J., Hastie, T.,Simon, N.,Taylor, J. and Tibshirani, Ryan. (2012) Strong Rules for Discarding Predictors in Lasso-type Problems, <em>JRSSB</em>, <b>Vol. 74(2)</b>, 245-266, https://statweb.stanford.edu/~tibs/ftp/strong.pdf.
</p>
<p>Hastie, T., Tibshirani, Robert and Tibshirani, Ryan. Extended Comparisons of Best Subset Selection, Forward Stepwise Selection, and the Lasso (2017), <em>Stanford Statistics Technical Report</em>, https://arxiv.org/abs/1707.08692.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Contructing the data
library(gamlss.lasso)
set.seed(123)
n&lt;- 500
d&lt;- 50
X&lt;- matrix(rnorm(n*d), n,d)
BETA&lt;- cbind( "mu"=rbinom(d,1,.1), "sigma"= rbinom(d,1,.1)*.3)
ysd&lt;- exp(1 + tcrossprod( BETA[,2],X))
data&lt;- cbind(y=as.numeric(rnorm(n, sd=ysd))+t(tcrossprod( BETA[,1],X)), as.data.frame(X))


# Estimating the model using default setting: adaptive lasso with BIC tuning
mod &lt;- gamlss(y~gnet(x.vars=names(data)[-1]),
              sigma.fo=~gnet(x.vars=names(data)[-1]), data=data,
              family=NO, i.control = glim.control(cyc=1, bf.cyc=1))

# Estimating the model with standard lasso (BIC tuning)
mod.lasso &lt;- gamlss(y~gnet(x.vars=names(data)[-1], adaptive=NULL),
              sigma.fo=~gnet(x.vars=names(data)[-1], adaptive=NULL), data=data, 
              family=NO, i.control = glim.control(cyc=1, bf.cyc=1))

# Estimated paramters are available at
rbind(true=BETA[,1],alasso=tail(getSmo(mod, "mu") ,1)[[1]]$beta,
                    lasso=tail(getSmo(mod.lasso, "mu") ,1)[[1]]$beta) ##beta for mu
rbind(true=BETA[,2],alasso=tail(getSmo(mod, "sigma") ,1)[[1]]$beta,
                    lasso=tail(getSmo(mod.lasso, "sigma") ,1)[[1]]$beta)##beta for sigma

# Estimating with other setting
nfolds&lt;- 6
n&lt;- dim(data)[1]
# folds for cross-validation and bootstrap
CVfolds&lt;- lapply(as.data.frame(t(sapply(sample(rep_len(1:nfolds,length=n),replace=FALSE)
                 ,"!=", 1:nfolds))), which)  
BOOTfolds&lt;- lapply(as.data.frame(matrix(sample(1:n, nfolds*n, replace=TRUE), n)),sort)  

#Bootstrap + Aggrationg = Bagging:
mod1 &lt;- gamlss(y~gnet(x.vars=names(data)[-1], method="CV",type="agg", subsets=BOOTfolds),
               sigma.fo=~gnet(x.vars=names(data)[-1]), data=data, family=NO,
               i.control = glim.control(cyc=1, bf.cyc=1)) 

# Estimated paramters are available at
tail(getSmo(mod1, "mu") ,1)[[1]]$beta ## beta for mu
tail(getSmo(mod1, "sigma") ,1)[[1]]$beta ## beta for sigma

# Cross-validation (with selection):
mod2 &lt;- gamlss(y~gnet(x.vars=names(data)[-1],method="CV",type="sel", subsets=CVfolds),
               sigma.fo=~gnet(x.vars=names(data)[-1],method="CV",type="sel", ICpen=2, 
               subsets=CVfolds), data=data, family=NO,
               i.control = glim.control(cyc=1, bf.cyc=1)) 

# Estimated paramters are available at
tail(getSmo(mod2, "mu") ,1)[[1]]$beta ## beta for mu
tail(getSmo(mod2, "sigma") ,1)[[1]]$beta ## beta for sigma


</code></pre>


</div>