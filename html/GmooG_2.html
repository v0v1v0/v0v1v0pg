<div class="container">

<table style="width: 100%;"><tr>
<td>aFacial</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Testing facial recognition software
</h2>

<h3>Description</h3>

<p>Buolamwini and Gebru used their own database that included more women and more people of colour to evaluate how well commercial gender classification algorithms coped with different shades of skin colour in a gender-balanced test database.
</p>


<h3>Usage</h3>

<pre><code class="language-R">data(aFacial)</code></pre>


<h3>Format</h3>

<p>A data frame with 72 observations on the following 5 variables.
</p>

<dl>
<dt><code>Sex</code></dt>
<dd>
<p>Female or Male</p>
</dd>
<dt><code>Skin</code></dt>
<dd>
<p>one of six shades of skin colour from I to VI</p>
</dd>
<dt><code>Prediction</code></dt>
<dd>
<p>Correct or Wrong</p>
</dd>
<dt><code>Freq</code></dt>
<dd>
<p>number of cases</p>
</dd>
<dt><code>Software</code></dt>
<dd>
<p>one of three facial recognition software packages</p>
</dd>
</dl>
<h3>Details</h3>

<p>Summary data tables of percentages and some numerical totals were provided in the paper and the supplementary material.  Assuming the results had to be based on integer numbers of cases it was possible to reconstruct summary raw numbers of the dataset.  The dataset is analysed in Chapter 22, "Comparing software for facial recognition".
</p>


<h3>Source</h3>

<p>Buolamwini, Joy, and Timnit Gebru. 2018. "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification." Proceedings of Machine Learning Research 81: 1-15
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(aFacial, package="GmooG")
head(aFacial, n=12)
</code></pre>


</div>