<div class="container">

<table style="width: 100%;"><tr>
<td>optimisers</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>optimisation methods</h2>

<h3>Description</h3>

<p>Functions to set up optimisers (which find parameters that
maximise the joint density of a model) and change their tuning parameters,
for use in <code>opt()</code>. For details of the algorithms and how to
tune them, see the
<a href="https://docs.scipy.org/doc/scipy/tutorial/optimize.html?highlight=optimize">SciPy optimiser docs</a> or the
<a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib">TensorFlow optimiser docs</a>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nelder_mead()

powell()

cg()

bfgs()

newton_cg()

l_bfgs_b(maxcor = 10, maxls = 20)

tnc(max_cg_it = -1, stepmx = 0, rescale = -1)

cobyla(rhobeg = 1)

slsqp()

gradient_descent(learning_rate = 0.01)

adadelta(learning_rate = 0.001, rho = 1, epsilon = 1e-08)

adagrad(learning_rate = 0.8, initial_accumulator_value = 0.1)

adagrad_da(
  learning_rate = 0.8,
  global_step = 1L,
  initial_gradient_squared_accumulator_value = 0.1,
  l1_regularization_strength = 0,
  l2_regularization_strength = 0
)

momentum(learning_rate = 0.001, momentum = 0.9, use_nesterov = TRUE)

adam(learning_rate = 0.1, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-08)

ftrl(
  learning_rate = 1,
  learning_rate_power = -0.5,
  initial_accumulator_value = 0.1,
  l1_regularization_strength = 0,
  l2_regularization_strength = 0
)

proximal_gradient_descent(
  learning_rate = 0.01,
  l1_regularization_strength = 0,
  l2_regularization_strength = 0
)

proximal_adagrad(
  learning_rate = 1,
  initial_accumulator_value = 0.1,
  l1_regularization_strength = 0,
  l2_regularization_strength = 0
)

rms_prop(learning_rate = 0.1, decay = 0.9, momentum = 0, epsilon = 1e-10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>maxcor</code></td>
<td>
<p>maximum number of 'variable metric corrections' used to define
the approximation to the hessian matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxls</code></td>
<td>
<p>maximum number of line search steps per iteration</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_cg_it</code></td>
<td>
<p>maximum number of hessian * vector evaluations per iteration</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stepmx</code></td>
<td>
<p>maximum step for the line search</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rescale</code></td>
<td>
<p>log10 scaling factor used to trigger rescaling of objective</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rhobeg</code></td>
<td>
<p>reasonable initial changes to the variables</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learning_rate</code></td>
<td>
<p>the size of steps (in parameter space) towards the
optimal value</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>the decay rate</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>a small constant used to condition gradient updates</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial_accumulator_value</code></td>
<td>
<p>initial value of the 'accumulator' used to
tune the algorithm</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>global_step</code></td>
<td>
<p>the current training step number</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial_gradient_squared_accumulator_value</code></td>
<td>
<p>initial value of the
accumulators used to tune the algorithm</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>l1_regularization_strength</code></td>
<td>
<p>L1 regularisation coefficient (must be 0 or
greater)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>l2_regularization_strength</code></td>
<td>
<p>L2 regularisation coefficient (must be 0 or
greater)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>momentum</code></td>
<td>
<p>the momentum of the algorithm</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_nesterov</code></td>
<td>
<p>whether to use Nesterov momentum</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta1</code></td>
<td>
<p>exponential decay rate for the 1st moment estimates</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta2</code></td>
<td>
<p>exponential decay rate for the 2nd moment estimates</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learning_rate_power</code></td>
<td>
<p>power on the learning rate, must be 0 or less</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>decay</code></td>
<td>
<p>discounting factor for the gradient</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The optimisers <code>powell()</code>, <code>cg()</code>, <code>newton_cg()</code>,
<code>l_bfgs_b()</code>, <code>tnc()</code>, <code>cobyla()</code>, and <code>slsqp()</code> are
deprecated. They will be removed in greta 0.4.0, since they will no longer
be available in TensorFlow 2.0, on which that version of greta will depend.
</p>
<p>The <code>cobyla()</code> does not provide information about the number of
iterations nor convergence, so these elements of the output are set to NA
</p>


<h3>Value</h3>

<p>an <code>optimiser</code> object that can be passed to <code>opt()</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# use optimisation to find the mean and sd of some data
x &lt;- rnorm(100, -2, 1.2)
mu &lt;- variable()
sd &lt;- variable(lower = 0)
distribution(x) &lt;- normal(mu, sd)
m &lt;- model(mu, sd)

# configure optimisers &amp; parameters via 'optimiser' argument to opt
opt_res &lt;- opt(m, optimiser = bfgs())

# compare results with the analytic solution
opt_res$par
c(mean(x), sd(x))

## End(Not run)
</code></pre>


</div>