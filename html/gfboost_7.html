<div class="container">

<table style="width: 100%;"><tr>
<td>LocRank</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Localized ranking familty</h2>

<h3>Description</h3>

<p>Gradient-free Gradient Boosting family for the localized ranking loss function including its fast
computation.
</p>


<h3>Usage</h3>

<pre><code class="language-R">LocRank(K)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>Indicates that we are interesting in the top <code class="reqn">K</code> instances and their correct ordering. Must be an integer
between 1 and the number <code class="reqn">n</code> of observations.</p>
</td>
</tr></table>
<h3>Details</h3>

<p>The localized ranking loss combines the hard and the weak ranking loss, i.e., it penalizes misrankings at
the top of the list (the best <code class="reqn">K</code> instances according to the response value) and ”misclassification” in the sense
that instances belonging to the top of the list are ranked lower and vice versa. The localized ranking loss already
returns a normalized loss that can take values between 0 and 1. <code>LocRank</code> returns a family object
as in the package <code>mboost</code>.
</p>


<h3>Value</h3>

<p>A Boosting family object
</p>


<h3>References</h3>

<p>Werner, T., Gradient-Free Gradient Boosting, PhD Thesis, Carl von Ossietzky University Oldenburg, 2020,
Equation (5.2.5)
</p>
<p>T. Hothorn, P. Bühlmann, T. Kneib, M. Schmid, and B. Hofner. mboost: Model-Based
Boosting, 2017
</p>


<h3>Examples</h3>

<pre><code class="language-R">{y&lt;-c(-3, 10.3,-8, 12, 14,-0.5, 29,-1.1,-5.7, 119)
 yhat&lt;-c(0.02, 0.6, 0.1, 0.47, 0.82, 0.04, 0.77, 0.09, 0.01, 0.79)
 LocRank(4)@risk(y,yhat)}
{y&lt;-c(-3, 10.3,-8, 12, 14,-0.5, 29,-1.1,-5.7, 119)
 yhat&lt;-c(0.02, 0.6, 0.1, 0.47, 0.82, 0.04, 0.77, 0.09, 0.01, 0.79)
 LocRank(5)@risk(y,yhat)}
</code></pre>


</div>