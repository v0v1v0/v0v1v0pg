<div class="container">

<table style="width: 100%;"><tr>
<td>performance.metrics</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Performance of model predictions</h2>

<h3>Description</h3>

<p>Calculate the performance of a model through a comparison 
between predicted and observed labels. Available metrics are <code>accuracy</code>,
<code>F1</code> and <code>TSS</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">performance.metrics(actual, predicted, metric)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>actual</code></td>
<td>
<p>dataframe. Same formatting as <code>y</code>, containg some sort of classification data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predicted</code></td>
<td>
<p>dataframe. Same formatting as <code>x</code>, containg the predicted classifications of a model trained over the data in <code>x</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>
<p>character. String specifying the metric used, one of <code>accuracy</code>, <code>F1</code> and <code>TSS</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><strong>The F-score or F-measure (F1)</strong> is: <br><br><code class="reqn">F1 = 2 \dfrac{Precision * Recall}{Precision + Recall}</code>, with <br><br><code class="reqn">Precision = \dfrac{True Positive}{True Positive + False Positive}</code> <br><br><code class="reqn">Recall = \dfrac{True Positive}{True Positive + False Negative}</code> <br><br><strong>Accuracy</strong> is: <br><br><code class="reqn">\dfrac{100 * (True Postives + True Negatives)}{True Postives + True Negatives + False Positives + False Negatives}</code>
<br><br><strong>The Pierce's skill score (PSS),  Bookmaker's Informedness (BM) or True Skill Statistic (TSS)</strong> is: <br><br><code class="reqn">TSS = TPR + TNR - 1</code>, <br>
with <code class="reqn">TPR</code> being the True Positive Rate, positives correctly labelled 
as such and <code class="reqn">TNR</code>, the True Negative Rate, the rate of negatives correctly
labelled, such that:<br><br><code class="reqn">TPR = \dfrac{True Positives}{True Positives + False Negatives}</code>
<br><code class="reqn">TNR = \dfrac{True Negatives}{True Negatives + False Positives}</code>
<br>
Take in consideration the fact that the F1 score is not a robust metric in datasets with class imbalances.
</p>


<h3>Value</h3>

<p>numeric.
</p>


<h3>References</h3>

<p>PSS:
Peirce, C. S. (1884). The numerical measure of the success of predictions. Science, 4, 453â€“454.
</p>


<h3>Examples</h3>

<pre><code class="language-R">observed = c("FALSE", "TRUE", "FALSE", "TRUE", "TRUE")
predicted = c("TRUE", "TRUE", "TRUE", "TRUE", "TRUE")
performance.metrics(observed, predicted, "TSS")
</code></pre>


</div>