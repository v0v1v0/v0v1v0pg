<div class="container">

<table style="width: 100%;"><tr>
<td>gp_optim</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Optimize hyperparameters of a GP model</h2>

<h3>Description</h3>

<p>This function can be used to optimize the hyperparameters of the model to the maximum
marginal likelihood (or maximum marginal posterior if priors are used), using Nelder-Mead
algorithm.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gp_optim(
  gp,
  x,
  y,
  tol = 1e-04,
  tol_param = 0.1,
  maxiter = 500,
  restarts = 1,
  verbose = TRUE,
  warnings = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>gp</code></td>
<td>
<p>The gp model object to be fitted.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>n-by-d matrix of input values (n is the number of observations and d the input
dimension). Can also be a vector of length n if the model has only a single input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Vector of n output (target) values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Relative change in the objective function value (marginal log posterior) after 
which the optimization is terminated. This will be passed to the function stats::optim 
as a convergence criterion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol_param</code></td>
<td>
<p>After the optimizer (Nelder-Mead) has terminated, the found hyperparameter
values will be checked for convergence within tolerance tol_param. More precisely, if we perturb
any of the hyperparameters by the amount tol_param or -tol_param, then the resulting
log posterior must be smaller than the value with the found hyperparameter values. If not,
then the optimizer will automatically attempt a restart (see argument restarts). Note:
tol_param will be applied for the logarithms of the parameters (e.g. log length-scale), not
for the native parameter values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiter</code></td>
<td>
<p>Maximum number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>restarts</code></td>
<td>
<p>Number of possible restarts during optimization. The Nelder-Mead 
iteration can sometimes terminate prematurely before a local optimum is found, and
this argument can be used to specify how many times the optimization is allowed to
restart from where it left when Nelder-Mead terminated. By setting restarts &gt; 0, 
one can often find local optimum without having to call gp_optim several times.
Note: usually there is no need to allow more than a few (say 1-3) restarts; 
if the optimization does not converge with a few restarts, then one usually 
must try to reduce argument tol in order to achieve convergence. If this does not help
either, then the optimization problem is usually ill-conditioned somehow.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If TRUE, then some information about the progress of the optimization is
printed to the console.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>warnings</code></td>
<td>
<p>Whether to print out some potential warnings (such as maximum number of
iterations reached) during the optimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Further arguments to be passed to <code>gp_fit</code> that are needed
in the fitting process, for example <code>trials</code> in the case of binomial likelihood.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An updated GP model object.
</p>


<h3>References</h3>

<p>Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian processes for machine learning. MIT Press.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Generate some toy data
set.seed(1242)
n &lt;- 50
x &lt;- matrix(rnorm(n * 3), nrow = n)
f &lt;- sin(x[, 1]) + 0.5 * x[, 2]^2 + x[, 3]
y &lt;- f + 0.5 * rnorm(n)
x &lt;- data.frame(x1 = x[, 1], x2 = x[, 2], x3 = x[, 3])

# Basic usage
cf &lt;- cf_sexp()
lik &lt;- lik_gaussian()
gp &lt;- gp_init(cf, lik)
gp &lt;- gp_optim(gp, x, y)


</code></pre>


</div>