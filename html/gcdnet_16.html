<div class="container">

<table style="width: 100%;"><tr>
<td>gcdnet</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fits the regularization paths for large margin classifiers</h2>

<h3>Description</h3>

<p>Fits a regularization path for large margin classifiers at a sequence of
regularization parameters lambda.
</p>


<h3>Usage</h3>

<pre><code class="language-R">gcdnet(
  x,
  y,
  nlambda = 100,
  method = c("hhsvm", "logit", "sqsvm", "ls", "er"),
  lambda.factor = ifelse(nobs &lt; nvars, 0.01, 1e-04),
  lambda = NULL,
  lambda2 = 0,
  pf = rep(1, nvars),
  pf2 = rep(1, nvars),
  exclude,
  dfmax = nvars + 1,
  pmax = min(dfmax * 1.2, nvars),
  standardize = FALSE,
  intercept = TRUE,
  eps = 1e-08,
  maxit = 1e+06,
  delta = 2,
  omega = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>matrix of predictors, of dimension <code class="reqn">N \times p</code>; each row
is an observation vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>response variable. This argument should be a two-level factor for
classification.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>the number of <code>lambda</code> values - default is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>a character string specifying the loss function to use, valid
options are: </p>
 <ul>
<li> <p><code>"hhsvm"</code> Huberized squared hinge loss,
</p>
</li>
<li> <p><code>"sqsvm"</code> Squared hinge loss, </p>
</li>
<li> <p><code>"logit"</code> logistic
loss, </p>
</li>
<li> <p><code>"ls"</code> least square loss. </p>
</li>
<li> <p><code>"er"</code> expectile
regression loss. </p>
</li>
</ul>
<p> Default is <code>"hhsvm"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.factor</code></td>
<td>
<p>The factor for getting the minimal lambda in
<code>lambda</code> sequence, where <code>min(lambda)</code> = <code>lambda.factor</code> *
<code>max(lambda)</code>, where <code>max(lambda)</code> is the smallest value of
<code>lambda</code> for which all coefficients are zero. The default depends on
the relationship between <code class="reqn">N</code> (the number of rows in the matrix of
predictors) and <code class="reqn">p</code> (the number of predictors). If <code class="reqn">N &gt; p</code>, the
default is <code>0.0001</code>, close to zero. If <code class="reqn">N&lt;p</code>, the default is
<code>0.01</code>. A very small value of <code>lambda.factor</code> will lead to a
saturated fit. It takes no effect if there is user-defined <code>lambda</code>
sequence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>a user supplied <code>lambda</code> sequence. Typically, by leaving
this option unspecified users can have the program compute its own
<code>lambda</code> sequence based on <code>nlambda</code> and <code>lambda.factor</code>.
Supplying a value of <code>lambda</code> overrides this. It is better to supply
a decreasing sequence of <code>lambda</code> values than a single (small) value,
if not, the program will sort user-defined <code>lambda</code> sequence in
decreasing order automatically.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda2</code></td>
<td>
<p>regularization parameter <code class="reqn">\lambda_2</code> for the
quadratic penalty of the coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pf</code></td>
<td>
<p>L1 penalty factor of length <code class="reqn">p</code> used for adaptive LASSO or
adaptive elastic net. Separate L1 penalty weights can be applied to each
coefficient of <code class="reqn">\beta</code> to allow differential L1 shrinkage. Can
be 0 for some variables, which implies no L1 shrinkage, and results in
that variable always being included in the model. Default is 1 for all
variables (and implicitly infinity for variables listed in
<code>exclude</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pf2</code></td>
<td>
<p>L2 penalty factor of length <code class="reqn">p</code> used for adaptive LASSO or
adaptive elastic net. Separate L2 penalty weights can be applied to each
coefficient of <code class="reqn">\beta</code> to allow differential L2 shrinkage. Can
be 0 for some variables, which implies no L2 shrinkage. Default is 1 for
all variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exclude</code></td>
<td>
<p>indices of variables to be excluded from the model. Default
is none. Equivalent to an infinite penalty factor.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfmax</code></td>
<td>
<p>limit the maximum number of variables in the model. Useful for
very large <code class="reqn">p</code>, if a partial path is desired. Default is <code class="reqn">p+1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pmax</code></td>
<td>
<p>limit the maximum number of variables ever to be nonzero. For
example once <code class="reqn">\beta</code> enters the model, no matter how many times it
exits or re-enters model through the path, it will be counted only once.
Default is <code>min(dfmax*1.2,p)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>logical flag for variable standardization, prior to
fitting the model sequence. If <code>TRUE</code>, <code>x</code> matrix is normalized
such that <code>x</code> is centered (i.e.
<code class="reqn">\sum^N_{i=1}x_{ij}=0</code>), and sum squares of each column
<code class="reqn">\sum^N_{i=1}x_{ij}^2/N=1</code>. If <code>x</code> matrix is
standardized, the ending coefficients will be transformed back to the
original scale. Default is <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>logical flag to indicate whether to include or exclude the
intercept in the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>convergence threshold for coordinate majorization descent. Each
inner coordinate majorization descent loop continues until the relative
change in any coefficient (i.e., <code class="reqn">\max_j(\beta_j^{new}
-\beta_j^{old})^2</code>) is less than
<code>eps</code>. For HHSVM, i.e., <code>method="hhsvm"</code>, it is
<code class="reqn">\frac{2}{\delta}\max_j(\beta_j^{new}-\beta_j^{old})^2</code>. For expectile regression,
i.e., <code>method="er"</code>, it is <code class="reqn">2\max(1-\omega,\omega)
\max_j(\beta_j^{new}-\beta_j^{old})^2</code>. Defaults value is <code>1e-8</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>maximum number of outer-loop iterations allowed at fixed lambda
value. Default is 1e6. If models do not converge, consider increasing
<code>maxit</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>the parameter <code class="reqn">\delta</code> in the HHSVM model. The value
must be greater than 0. Default is 2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>omega</code></td>
<td>
<p>the parameter <code class="reqn">\omega</code> in the expectile regression
model. The value must be in (0,1). Default is 0.5.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Note that the objective function in <code>gcdnet</code> is </p>
<p style="text-align: center;"><code class="reqn">Loss(y,
  X, \beta)/N + \lambda_1\Vert\beta\Vert_1 +
  0.5\lambda_2\Vert\beta\Vert_2^2</code>
</p>
<p> where the penalty is a combination of L1 and L2
term. Users can specify the loss function to use, options include
Huberized squared hinge loss, Squared hinge loss, least square loss,
logistic regression and expectile regression loss. Users can also tweak
the penalty by choosing different <code class="reqn">lambda2</code> and penalty factor.
</p>
<p>For computing speed reason, if models are not converging or running slow,
consider increasing <code>eps</code>, decreasing <code>nlambda</code>, or increasing
<code>lambda.factor</code> before increasing <code>maxit</code>.
</p>
<p><strong>FAQ:</strong>
</p>
<p><b>Question: </b>“<em>I couldn't get an idea how to specify an
option to get adaptive LASSO, how to specify an option to get elastic net
and adaptive elastic net? Could you please give me a quick hint?</em>”
</p>
<p><b>Answer: </b> <code>lambda2</code> is the regularize parameter for L2 penalty
part. To use LASSO, set <code>lambda2=0</code>. To use elastic net, set
<code>lambda2</code> as nonzero.
</p>
<p><code>pf</code> is the L1 penalty factor of length <code class="reqn">p</code> (<code class="reqn">p</code> is the
number of predictors). Separate L1 penalty weights can be applied to each
coefficient to allow differential L1 shrinkage. Similiarly <code>pf2</code> is
the L2 penalty factor of length <code class="reqn">p</code>.
</p>
<p>To use adaptive LASSO, you should set <code>lambda2=0</code> and also specify
<code>pf</code> and <code>pf2</code>. To use adaptive elastic net, you should set
<code>lambda2</code> as nonzero and specify <code>pf</code> and <code>pf2</code>,
</p>
<p>For example:
</p>
<pre>
    library('gcdnet')
    # Dataset N = 100, p = 10
    x_log &lt;- matrix(rnorm(100*10),100,10)
    y_log &lt;- sample(c(-1,1),100,replace=TRUE)

    # LASSO
    m &lt;- gcdnet(x=x_log,y=y_log,lambda2=0,method="log")
    plot(m)

    # elastic net with lambda2 = 1
    m &lt;- gcdnet(x=x_log,y=y_log,lambda2=1,method="log")
    plot(m)

    # adaptive lasso with penalty factor
    # pf = 0.5 0.5 0.5 0.5 0.5 1.0 1.0 1.0 1.0 1.0
    m &lt;- gcdnet(x=x_log,y=y_log,lambda2=0,method="log",
                pf=c(rep(0.5,5),rep(1,5)))
    plot(m)

    # adaptive elastic net with lambda2 = 1 and penalty factor pf =
    # c(rep(0.5,5),rep(1,5)) pf2 = 3 3 3 3 3 1 1 1 1 1
    m &lt;- gcdnet(x=x_log,y=y_log,lambda2=1,method="log",
                pf=c(rep(0.5,5),rep(1,5)),
                pf2 = c(rep(3,5),rep(1,5)))
    plot(m)
  </pre>
<p><b>Question: </b>“<em>what is the meaning of the parameter
<code>pf</code>? On the package documentation, it said <code>pf</code> is the penalty
weight applied to each coefficient of beta?</em>”
</p>
<p><b>Answer: </b> Yes, <code>pf</code> and <code>pf2</code> are L1 and L2 penalty factor
of length <code class="reqn">p</code> used for adaptive LASSO or adaptive elastic net. 0
means that the feature (variable) is always excluded, 1 means that the
feature (variable) is included with weight 1.
</p>
<p><b>Question: </b>“<em>Does gcdnet deal with both continuous and
categorical response variables?</em>”
</p>
<p><b>Answer: </b> Yes, both are supported, you can use a continuous type
response variable with the least squares regression loss, or a categorical
type response with losses for classification problem.
</p>
<p><b>Question: </b>“<em>Why does predict function not work? predict
should return the predicted probability of the positive class. Instead I
get:</em>”
</p>
<pre>
    Error in as.matrix(as.matrix(cbind2(1, newx)) %*% nbeta):
    error in evaluating the argument 'x' in selecting a method for function 'as.matrix':
    Error in t(.Call(Csparse_dense_crossprod, y, t(x))):
    error in evaluating the argument 'x' in selecting a method for function 't':
    Error: Cholmod error 'X and/or Y have wrong dimensions' at
      file ../MatrixOps/cholmod_sdmult.c, line 90?
  </pre>
<p>“<em>Using the Arcene dataset and executing the following code
will give the above error:</em>”
</p>
<pre>
    library(gcdnet)
    arc &lt;- read.csv("arcene.csv", header=FALSE)
    fit &lt;- gcdnet(arc[,-10001], arc[,10001], standardize=FALSE,
                  method="logit")
    pred &lt;- rnorm(10000)
    predict(fit, pred, type="link")
  </pre>
<p><b>Answer: </b> It is actually NOT a bug of gcdnet. When make prediction
using a new matrix x, each observation of x should be arranged as a row of
a matrix. In your code, because "pred" is a vector, you need to convert
"pred" into a matrix, try the following code:
</p>
<pre>
     pred &lt;- rnorm(10000)
     pred &lt;- matrix(pred,1,10000)
     predict(fit, pred, type="link")
  </pre>


<h3>Value</h3>

<p>An object with S3 class <code>gcdnet</code>. </p>
<table>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the call
that produced this object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b0</code></td>
<td>
<p>intercept sequence of length
<code>length(lambda)</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>a <code>p*length(lambda)</code> matrix of
coefficients, stored as a sparse matrix (<code>dgCMatrix</code> class, the
standard class for sparse numeric matrices in the <code>Matrix</code> package.).
To convert it into normal type matrix use <code>as.matrix()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>the actual sequence of <code>lambda</code> values used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>the number of nonzero coefficients for each value of
<code>lambda</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>dimension of coefficient matrix (ices)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>npasses</code></td>
<td>
<p>total number of iterations (the most inner loop) summed
over all lambda values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jerr</code></td>
<td>
<p>error flag, for warnings and errors, 0
if no error.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Yi Yang, Yuwen Gu and Hui Zou<br>
Maintainer: Yi Yang &lt;yi.yang6@mcgill.ca&gt;
</p>


<h3>References</h3>

<p>Yang, Y. and Zou, H. (2012).
"An Efficient Algorithm for Computing The HHSVM and Its Generalizations."
<em>Journal of Computational and Graphical Statistics</em>, 22, 396-415.<br>
BugReport: <a href="https://github.com/emeryyi/gcdnet">https://github.com/emeryyi/gcdnet</a><br></p>
<p>Gu, Y., and Zou, H. (2016).
"High-dimensional generalizations of asymmetric least squares regression and their applications."
<em>The Annals of Statistics</em>, 44(6), 2661–2694.<br></p>


<h3>See Also</h3>

<p><code>plot.gcdnet</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data(FHT)
# 1. solution paths for the LASSO penalized least squares.
# To use LASSO set lambda2 = 0.

m1 &lt;- gcdnet(x = FHT$x, y = FHT$y_reg, lambda2 = 0, method = "ls")
plot(m1)

# 2. solution paths for the elastic net penalized HHSVM.
# lambda2 is the parameter controlling the L2 penalty.
m2 &lt;- gcdnet(x = FHT$x, y = FHT$y, delta = 1, lambda2 = 1, method = "hhsvm")
plot(m2)

# 3. solution paths for the adaptive LASSO penalized SVM
# with the squared hinge loss. To use the adaptive LASSO,
# set lambda2 = 0 and meanwhile specify the L1 penalty weights.
p &lt;- ncol(FHT$x)
# set the first three L1 penalty weights as 0.1 and the rest are 1
pf = c(0.1, 0.1, 0.1, rep(1, p-3))
m3 &lt;- gcdnet(x = FHT$x, y = FHT$y, pf = pf, lambda2 = 0, method = "sqsvm")
plot(m3)

# 4. solution paths for the adaptive elastic net penalized
# logistic regression.

p &lt;- ncol(FHT$x)
# set the first three L1 penalty weights as 10 and the rest are 1.
pf &lt;- c(10, 10, 10, rep(1, p-3))
# set the last three L2 penalty weights as 0.1 and the rest are 1.
pf2 &lt;- c(rep(1, p-3), 0.1, 0.1, 0.1)
# set the L2 penalty parameter lambda2=0.01.
m4 &lt;- gcdnet(x = FHT$x, y = FHT$y, pf = pf, pf2 = pf2,
             lambda2 = 0.01, method = "logit")
plot(m4)

# 5. solution paths for the LASSO penalized expectile regression
# with the asymmetric least square parameter omega=0.9.

m5 &lt;- gcdnet(x = FHT$x, y = FHT$y_reg, omega = 0.9,
             lambda2 = 0, method = "er")
plot(m5)

</code></pre>


</div>